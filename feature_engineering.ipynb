{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\tkhoon\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "##构造libffm所需要的dataformat\n",
    "import pandas  as pd\n",
    "from tqdm import tqdm\n",
    "##怎么划分训练集和测试集\n",
    "##训练集的要求 1 尽量包含完所有搜索词  先随便来试一下哦，不要管那么多\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "train_data = pd.read_table('E:/competionfile/oppo/new/oppo_round1_train_20180929/oppo_round1_train_20180929.txt',names=[0,1,2,3,4],header=None,encoding='utf8').astype(str)\n",
    "train_data = train_data[ train_data[4] != '音乐' ]\n",
    "test_data  = pd.read_table('E:/competionfile/oppo/new/oppo_round1_train_20180929/oppo_round1_vali_20180929.txt',names=[0,1,2,3,4],header=None,encoding='utf8').astype(str)\n",
    "\n",
    "train_data.columns = ['prefix','query_prediction','title','tag','label']\n",
    "test_data.columns = ['prefix','query_prediction','title','tag','label']\n",
    "train_data[ 'label' ] = train_data['label'].astype(int)\n",
    "train_data['realtitle'] = train_data['title']+train_data['tag']\n",
    "test_data['realtitle'] = test_data['title']+test_data['tag']\n",
    "\n",
    "train_data = train_data.reset_index()\n",
    "train_data = train_data.drop(['index'],axis=1)\n",
    "test_data = test_data.reset_index()\n",
    "test_data = test_data.drop(['index'],axis=1)\n",
    "\n",
    "# real_test_data = pd.read_table('E:/competionfile/oppo/new/oppo_round1_train_20180929/oppo_round1_test_A_20180929.txt',header=None,encoding='utf8').astype(str)\n",
    "# real_test_data.columns = ['prefix','query_prediction','title','tag']\n",
    "# real_test_data['realtitle']=real_test_data['title']+real_test_data['tag']\n",
    "# train_data['realtitle'] = train_data['title']+train_data['tag']\n",
    "\n",
    "# train_data['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\tkhoon\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:6201: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'querydict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-1785fed2911c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mwaitsimlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mquerytitlegroup\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mwaitsimlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mquerydict\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m##将waitsimlis分成4分\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'querydict' is not defined"
     ]
    }
   ],
   "source": [
    "##计算搜索预测词中最大的前两个搜索词和文章名字的相似度  ( 这个特征重新弄一下， 不用百度的familar 来做了，换成词向量和句向量的方式 )\n",
    "totaldata = train_data.append( test_data , ignore_index = True  )\n",
    "totaldata = totaldata.append( real_test_data , ignore_index = True  )\n",
    "\n",
    "totaldata['query']=totaldata['prefix']\n",
    "totaldata['prediction'] = totaldata['query_prediction']\n",
    "totaldata['realname'] = totaldata['title']+totaldata['tag']\n",
    "querytitlegroup = totaldata.groupby( ['query','realname'] )\n",
    "waitsimlist = []\n",
    "for name ,group in querytitlegroup:\n",
    "    waitsimlist.append( ( name , querydict[ name[0] ]) )\n",
    "    \n",
    "##将waitsimlis分成4分\n",
    "print(int(len(waitsimlist)/4))\n",
    "waitsimlist1 = waitsimlist[0:int(len(waitsimlist)/4)]\n",
    "waitsimlist2 = waitsimlist[int(len(waitsimlist)/4):int(len(waitsimlist)/2)]\n",
    "waitsimlist3 = waitsimlist[int(len(waitsimlist)/2):int(len(waitsimlist)/4*3)]\n",
    "waitsimlist4 = waitsimlist[int(len(waitsimlist)/4*3):len(waitsimlist)]\n",
    "\n",
    "f = open( 'e:/waitsimlist1.txt','w',encoding='utf-8')\n",
    "for item in waitsimlist:\n",
    "    f.writelines(  item[0][0] +'&&&&&'+item[0][1]+'&&&&&'+item[1]+'\\n'  )\n",
    "f.close()\n",
    "# f = open( 'e:/waitsimlist2.txt','w',encoding='utf-8')\n",
    "# for item in waitsimlist2:\n",
    "#     f.writelines(  item[0][0] +'&&&&&'+item[0][1]+'&&&&&'+item[1]+'\\n'  )\n",
    "# f.close()\n",
    "# f = open( 'e:/waitsimlist3.txt','w',encoding='utf-8')\n",
    "# for item in waitsimlist3:\n",
    "#     f.writelines(  item[0][0] +'&&&&&'+item[0][1]+'&&&&&'+item[1]+'\\n'  )\n",
    "# f.close()\n",
    "# f = open( 'e:/waitsimlist4.txt','w',encoding='utf-8')\n",
    "# for item in waitsimlist4:\n",
    "#     f.writelines(  item[0][0] +'&&&&&'+item[0][1]+'&&&&&'+item[1]+'\\n'  )\n",
    "f.close()\n",
    "\n",
    "##放入familiar文件中进行预测\n",
    "\n",
    "##文件路径 C:\\Users\\tkhoon\\Documents\\oppo比赛\\query_doc_sim_demo.py\n",
    "\n",
    "\n",
    "##将预测出来的相似度的值填入到原来的data中\n",
    "# import pickle \n",
    "# import sys\n",
    "# # reload(sys)\n",
    "# # sys.setdefaultencoding( 'utf-8' )\n",
    "# def ccacdis( cdismap , prefix ):\n",
    "#     dismap = cdismap\n",
    "#     f= open('e:/result/'+prefix+'result1.pickle','rb')\n",
    "#     resultlist = pickle.load(f)\n",
    "#     for row in resultlist:\n",
    "#     #     print(row[0])\n",
    "#         dismap[ (row[0],row[1]) ]=row[2]\n",
    "\n",
    "#     f= open('e:/result/'+prefix+'result2.pickle','rb')\n",
    "#     resultlist = pickle.load(f)\n",
    "#     for row in resultlist:\n",
    "#     #     print(row[0])\n",
    "#         dismap[ (row[0],row[1]) ]=row[2]\n",
    "#     f= open('e:/result/'+prefix+'result3.pickle','rb')\n",
    "#     resultlist = pickle.load(f)\n",
    "#     for row in resultlist:\n",
    "#     #     print(row[0])\n",
    "#         dismap[ (row[0],row[1]) ]=row[2]\n",
    "\n",
    "#     f= open('e:/result/'+prefix+'result4.pickle','rb')\n",
    "#     resultlist = pickle.load(f)\n",
    "#     for row in resultlist:\n",
    "#     #     print(row[0])\n",
    "#         dismap[ (row[0],row[1]) ]=row[2]\n",
    "        \n",
    "\n",
    "# dismap={}\n",
    "# newsdismap={}\n",
    "# weibodismap={}\n",
    "# ccacdis(dismap,'')\n",
    "# ccacdis(newsdismap,'news')\n",
    "# ccacdis(weibodismap,'weibo')\n",
    "\n",
    "# data['query'] = data[0]\n",
    "# data['realname'] = data[2]+data[3]\n",
    "# cdata['query'] = cdata[0]\n",
    "# cdata['realname'] = cdata[2]+cdata[3]\n",
    "# def getDis( row ):\n",
    "#     if dismap.get(( row['query'] , row['realname'] )) is not None:\n",
    "#         return (dismap.get(( row['query'] , row['realname'] ))+newsdismap.get(( row['query'] , row['realname'] ))+weibodismap.get(( row['query'] , row['realname'] )))/3\n",
    "#     else:\n",
    "#         return 0\n",
    "# data['avgpredictwzdis']= data.apply( getDis ,axis = 1 )\n",
    "# cdata['avgpredictwzdis'] = cdata.apply( getDis ,axis = 1 )\n",
    "# data['avgpredictwzdis'].to_csv( 'e:/avgpredictwzdis.csv', index=False )\n",
    "# cdata['avgpredictwzdis'].to_csv( 'e:/cavgcpredictwzdis.csv', index=False )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# namemap.get('猴儿酒百科')\n",
    "# for name ,group in tqdm(zzk):\n",
    "#     nameset = set( group['prefix'].value_counts().index )\n",
    "#     namemap[ name ] = nameset\n",
    "#     if name == '猴儿酒百科':\n",
    "#         print(name)\n",
    "#     print(name)\n",
    "# nameset = set( zzk )\n",
    "\n",
    "\n",
    "# totaldata[ totaldata['realtitle'] == '猴儿酒百科' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 135350/135350 [03:45<00:00, 601.05it/s]\n"
     ]
    }
   ],
   "source": [
    "##kk( 计算当前数据中预测词中概率最高的那一个和曾今点过该文章的所有搜索词中相似度最高的搜索词的相似度 )\n",
    "##原名叫kk 现在改名为 pre_titile_pre_difsim \n",
    "totaldata = train_data[ train_data['label'] == 1  ]\n",
    "zzk=totaldata.groupby('realtitle')\n",
    "namemap={}\n",
    "for name ,group in tqdm(zzk):\n",
    "    nameset = set( group['prefix'].value_counts().index )\n",
    "    namemap[ name ] = nameset\n",
    "\n",
    "\n",
    "import difflib\n",
    "import Levenshtein\n",
    "def latt( s1 , s2 ):\n",
    "    return Levenshtein.distance(s1,s2)\n",
    "def get_equal_rate_1(str1,str2):\n",
    "    return difflib.SequenceMatcher(None, str1, str2).quick_ratio()\n",
    "\n",
    "def findmax( item ):\n",
    "    query = item['prefix']\n",
    "    wzname = item['realtitle']\n",
    "    maxrate = 0\n",
    "    item_prefix_map = namemap.get(wzname)\n",
    "    if item_prefix_map is None:\n",
    "        return None\n",
    "    for i in item_prefix_map:\n",
    "        if query !=i:\n",
    "            currate = get_equal_rate_1( query , i )\n",
    "            if currate > maxrate:\n",
    "                maxrate = currate\n",
    "    if maxrate == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return maxrate\n",
    "# pre_titile_pre_difsim = train_data.apply( findmax ,axis=1)\n",
    "# test_pre_titile_pre_difsim = test_data.apply( findmax ,axis=1)\n",
    "# pre_titile_pre_difsim.to_csv('E:/competionfile/oppo/data/singleFeature/pre_titile_pre_difsim.csv',index=False)\n",
    "# test_pre_titile_pre_difsim.to_csv('E:/competionfile/oppo/data/singleFeature/test_pre_titile_pre_difsim.csv',index=False)\n",
    "# realtest_pre_titile_pre_difsim = real_test_data.apply( findmax ,axis=1)\n",
    "# realtest_pre_titile_pre_difsim.to_csv('E:/competionfile/oppo/data/singleFeature/real_test_pre_titile_pre_difsim.csv',index=False)\n",
    "# ##原名叫ykk 现在改名为 pre_titile_pre_latsim \n",
    "\n",
    "def findmin( item ):\n",
    "    query = item['prefix']\n",
    "    wzname = item['realtitle']\n",
    "    minrate = 30\n",
    "    find = False\n",
    "    item_prefix_map = namemap.get(wzname)\n",
    "    if item_prefix_map is None:\n",
    "        return None\n",
    "    for i in item_prefix_map:\n",
    "        if query !=i:\n",
    "            find =True\n",
    "            currate = latt( query , i )\n",
    "            if currate < minrate:\n",
    "                minrate = currate\n",
    "    if not find:\n",
    "        return None\n",
    "    else:\n",
    "        return minrate\n",
    "# pre_titile_pre_latsim = train_data.apply( findmin ,axis=1)\n",
    "# test_pre_titile_pre_latsim = test_data.apply( findmin ,axis=1)\n",
    "# pre_titile_pre_latsim.to_csv('E:/competionfile/oppo/data/singleFeature/pre_titile_pre_latsim.csv',index=False)\n",
    "# test_pre_titile_pre_latsim.to_csv('E:/competionfile/oppo/data/singleFeature/test_pre_titile_pre_latsim.csv',index=False)\n",
    "real_test_pre_titile_pre_latsim = real_test_data.apply( findmin ,axis=1)\n",
    "real_test_pre_titile_pre_latsim.to_csv('E:/competionfile/oppo/data/singleFeature/real_test_pre_titile_pre_latsim.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1044182, 6)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[ pre_titile_pre_difsim.isnull()  ].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur iter0\n",
      "alpha 0.768367174172722 beta1.1623332364159844\n",
      "cur iter1\n",
      "alpha 0.7082988736793424 beta1.1747580655767527\n",
      "cur iter2\n",
      "alpha 0.6841799084950175 beta1.1565769287913952\n",
      "cur iter3\n",
      "alpha 0.6683341442991793 beta1.133255153362522\n",
      "cur iter4\n",
      "alpha 0.6549118536757954 beta1.1100965688858957\n",
      "cur iter5\n",
      "alpha 0.6426356128151918 beta1.0881374101568755\n",
      "cur iter6\n",
      "alpha 0.6311949944216394 beta1.067517898650928\n",
      "cur iter7\n",
      "alpha 0.620485876477469 beta1.0481950187832472\n",
      "cur iter8\n",
      "alpha 0.610450029337446 beta1.0300923177901027\n",
      "cur iter9\n",
      "alpha 0.6010412814832309 beta1.0131309927906256\n",
      "cur iter10\n",
      "alpha 0.5922182835982339 beta0.997236070539907\n",
      "cur iter11\n",
      "alpha 0.5839428454182138 beta0.9823373986744003\n",
      "cur iter12\n",
      "alpha 0.5761794427384144 beta0.9683695745297554\n",
      "cur iter13\n",
      "alpha 0.5688949764835391 beta0.9552716707947323\n",
      "cur iter14\n",
      "alpha 0.5620585928399908 beta0.9429869355412398\n",
      "cur iter15\n",
      "alpha 0.5556415245937453 beta0.9314625027245473\n",
      "cur iter16\n",
      "alpha 0.5496169450020107 beta0.9206491198418009\n",
      "cur iter17\n",
      "alpha 0.5439598319526603 beta0.9105008934020404\n",
      "cur iter18\n",
      "alpha 0.5386468416094534 beta0.9009750516674577\n",
      "cur iter19\n",
      "alpha 0.5336561908092274 beta0.8920317236938257\n",
      "cur iter20\n",
      "alpha 0.5289675477450436 beta0.8836337338138555\n",
      "cur iter21\n",
      "alpha 0.5245619303890121 beta0.8757464106540741\n",
      "cur iter22\n",
      "alpha 0.5204216121514396 beta0.8683374097480916\n",
      "cur iter23\n",
      "alpha 0.5165300342737204 beta0.8613765488806431\n",
      "cur iter24\n",
      "alpha 0.5128717245655784 beta0.8548356553815448\n",
      "cur iter25\n",
      "alpha 0.5094322219437623 beta0.8486884244548133\n",
      "cur iter26\n",
      "alpha 0.5061980064860628 beta0.842910287880946\n",
      "cur iter27\n",
      "alpha 0.5031564344983809 beta0.8374782923134563\n",
      "cur iter28\n",
      "alpha 0.5002956783226281 beta0.8323709864955049\n",
      "cur iter29\n",
      "alpha 0.4976046704607135 beta0.8275683167210581\n",
      "cur iter30\n",
      "alpha 0.4950730517789812 beta0.8230515300210284\n",
      "cur iter31\n",
      "alpha 0.4926911234298745 beta0.818803084376748\n",
      "cur iter32\n",
      "alpha 0.49044980223762197 beta0.8148065655459513\n",
      "cur iter33\n",
      "alpha 0.48834057928985247 beta0.8110466099508465\n",
      "cur iter34\n",
      "alpha 0.48635548150687036 beta0.8075088332146988\n",
      "cur iter35\n",
      "alpha 0.4844870359514721 beta0.8041797638218074\n",
      "cur iter36\n",
      "alpha 0.48272823665156145 beta0.8010467816564996\n",
      "cur iter37\n",
      "alpha 0.4810725138071782 beta0.7980980608977226\n",
      "cur iter38\n",
      "alpha 0.4795137051476014 beta0.7953225170929732\n",
      "cur iter39\n",
      "alpha 0.47804602930651935 beta0.7927097579569674\n",
      "cur iter40\n",
      "alpha 0.4766640610481405 beta0.7902500376607763\n",
      "cur iter41\n",
      "alpha 0.47536270821623633 beta0.7879342144244753\n",
      "cur iter42\n",
      "alpha 0.4741371902919328 beta0.7857537110298899\n",
      "cur iter43\n",
      "alpha 0.4729830183957495 beta0.7837004781018927\n",
      "cur iter44\n",
      "alpha 0.47189597668741284 beta0.781766960002337\n",
      "cur iter45\n",
      "alpha 0.4708721050010138 beta0.7799460629967231\n",
      "cur iter46\n",
      "alpha 0.46990768268575533 beta0.7782311257058184\n",
      "cur iter47\n",
      "alpha 0.4689992134982278 beta0.7766158914905388\n",
      "cur iter48\n",
      "alpha 0.46814341155469447 beta0.7750944827979479\n",
      "cur iter49\n",
      "alpha 0.46733718813842173 beta0.7736613771605102\n",
      "cur iter50\n",
      "alpha 0.46657763947879427 beta0.7723113848971254\n",
      "cur iter51\n",
      "alpha 0.46586203525670644 beta0.7710396282271286\n",
      "cur iter52\n",
      "alpha 0.4651878078856098 beta0.769841521800826\n",
      "cur iter53\n",
      "alpha 0.46455254250538625 beta0.7687127545452812\n",
      "cur iter54\n",
      "alpha 0.463953967612402 beta0.7676492726668535\n",
      "cur iter55\n",
      "alpha 0.46338994629625024 beta0.7666472637774756\n",
      "cur iter56\n",
      "alpha 0.4628584680177552 beta0.7657031420400494\n",
      "cur iter57\n",
      "alpha 0.46235764095541976 beta0.7648135343155528\n",
      "cur iter58\n",
      "alpha 0.46188568479262604 beta0.7639752671454813\n",
      "cur iter59\n",
      "alpha 0.4614409239921161 beta0.7631853546193681\n",
      "cur iter60\n",
      "alpha 0.4610217814812645 beta0.7624409869710438\n",
      "cur iter61\n",
      "alpha 0.4606267727288742 beta0.7617395199415008\n",
      "cur iter62\n",
      "alpha 0.4602545002001321 beta0.7610784647675236\n",
      "cur iter63\n",
      "alpha 0.4599036481641346 beta0.7604554788524637\n",
      "cur iter64\n",
      "alpha 0.4595729778004083 beta0.7598683569642104\n",
      "cur iter65\n",
      "alpha 0.4592613226119012 beta0.7593150230205481\n",
      "cur iter66\n",
      "alpha 0.4589675841510751 beta0.7587935223702693\n",
      "cur iter67\n",
      "alpha 0.4586907279600058 beta0.758302014565639\n",
      "cur iter68\n",
      "alpha 0.4584297798030634 beta0.7578387665519652\n",
      "cur iter69\n",
      "alpha 0.45818382209395875 beta0.7574021463065368\n",
      "cur iter70\n",
      "alpha 0.4579519905624038 beta0.7569906168460342\n",
      "cur iter71\n",
      "alpha 0.4577334711073179 beta0.7566027306046827\n",
      "cur iter72\n",
      "alpha 0.4575274968579777 beta0.7562371241654612\n",
      "cur iter73\n",
      "alpha 0.4573333453940042 beta0.7558925132936078\n",
      "cur iter74\n",
      "alpha 0.45715033612406264 beta0.755567688257794\n",
      "cur iter75\n",
      "alpha 0.45697782787067803 beta0.7552615094932723\n",
      "cur iter76\n",
      "alpha 0.45681521653530416 beta0.7549729034455103\n",
      "cur iter77\n",
      "alpha 0.45666193293875396 beta0.7547008587223185\n",
      "cur iter78\n",
      "alpha 0.4565174407988521 beta0.7544444224694765\n",
      "cur iter79\n",
      "alpha 0.4563812347931356 beta0.7542026969088699\n",
      "cur iter80\n",
      "alpha 0.4562528387716724 beta0.7539748361644162\n",
      "cur iter81\n",
      "alpha 0.4561318040497532 beta0.7537600432207604\n",
      "cur iter82\n",
      "alpha 0.4560177078155341 beta0.7535575670612118\n",
      "cur iter83\n",
      "alpha 0.45591015162441395 beta0.7533667000078017\n",
      "cur iter84\n",
      "alpha 0.45580875999117376 beta0.7531867752096633\n",
      "cur iter85\n",
      "alpha 0.4557131790543813 beta0.7530171642350811\n",
      "cur iter86\n",
      "alpha 0.45562307532749874 beta0.752857274894099\n",
      "cur iter87\n",
      "alpha 0.4555381345152309 beta0.7527065490914264\n",
      "cur iter88\n",
      "alpha 0.455458060406336 beta0.7525644608681522\n",
      "cur iter89\n",
      "alpha 0.45538257381775865 beta0.752430514544104\n",
      "cur iter90\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-2d898acf563c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[0mbs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBayesianSmoothing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m \u001b[0mbs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcountlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclicklist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0000000001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;31m# ctrmap = {}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-2d898acf563c>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, imps, clks, iter_num, epsilon)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cur iter'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mnew_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_beta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__fixed_point_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'alpha {} beta{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mnew_alpha\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mnew_beta\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_alpha\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_beta\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-2d898acf563c>\u001b[0m in \u001b[0;36m__fixed_point_iteration\u001b[1;34m(self, imps, clks, alpha, beta)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mnumerator_alpha\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mspecial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdigamma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mspecial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdigamma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0mnumerator_beta\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mspecial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdigamma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mclks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mspecial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdigamma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m             \u001b[0mdenominator\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mspecial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdigamma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mspecial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdigamma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumerator_alpha\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mdenominator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumerator_beta\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mdenominator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## 高级ctr 对原始ctr进行了点击率平滑过后的点击率:\n",
    "#!/usr/bin/python\n",
    "# coding=utf-8\n",
    "##这里是用贝叶斯进行点击率平滑\n",
    "import numpy\n",
    "import random\n",
    "import scipy.special as special\n",
    " \n",
    "class BayesianSmoothing(object):\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    " \n",
    "    def sample(self, alpha, beta, num, imp_upperbound):\n",
    "        sample = numpy.random.beta(alpha, beta, num)\n",
    "#         print(sample)\n",
    "        I = []\n",
    "        C = []\n",
    "        for clk_rt in sample:\n",
    "            imp = imp_upperbound\n",
    "            clk = imp * clk_rt\n",
    "            I.append(imp)\n",
    "            C.append(clk)\n",
    "#         print(C)\n",
    "        return I, C\n",
    " \n",
    "    def update(self, imps, clks, iter_num, epsilon):\n",
    "        for i in range(iter_num):\n",
    "            print('cur iter'+str(i))\n",
    "            new_alpha, new_beta = self.__fixed_point_iteration(imps, clks, self.alpha, self.beta)\n",
    "            print('alpha {} beta{}'.format( new_alpha , new_beta ))\n",
    "            if abs(new_alpha-self.alpha)<epsilon and abs(new_beta-self.beta)<epsilon:\n",
    "                break\n",
    "            self.alpha = new_alpha\n",
    "            self.beta = new_beta\n",
    " \n",
    "    def __fixed_point_iteration(self, imps, clks, alpha, beta):\n",
    "        numerator_alpha = 0.0\n",
    "        numerator_beta = 0.0\n",
    "        denominator = 0.0\n",
    "        for i in range(len(imps)):\n",
    "            numerator_alpha += (special.digamma(clks[i]+alpha) - special.digamma(alpha))\n",
    "            numerator_beta += (special.digamma(imps[i]-clks[i]+beta) - special.digamma(beta))\n",
    "            denominator += (special.digamma(imps[i]+alpha+beta) - special.digamma(alpha+beta))\n",
    "        return alpha*(numerator_alpha/denominator), beta*(numerator_beta/denominator)\n",
    "train_data['realtitle'] = train_data['title']+train_data['tag']\n",
    "kkk = train_data.groupby(['prefix','realtitle'])\n",
    "namelist = []\n",
    "clicklist = []\n",
    "countlist = []\n",
    "for name , group in kkk:\n",
    "    namelist.append(name) \n",
    "    clicklist.append( len( group['label'][group['label'].astype(int) == 1]  ) ) \n",
    "    countlist.append(len( group ))\n",
    "\n",
    "bs = BayesianSmoothing(1, 1)\n",
    "bs.update(countlist, clicklist, 1000, 0.0000000001)\n",
    "\n",
    "# ctrmap = {}\n",
    "# alpha = 0.45415064089651735 \n",
    "# beta = 0.7502334280754297\n",
    "# for i in range(len(clicklist)):\n",
    "#         ctrmap[namelist[i]] =  (clicklist[i]+alpha)/(countlist[i]+alpha+beta) \n",
    "\n",
    "# ##将平滑后的概率重新注入进去\n",
    "# # train_data['realtitle'] = train_data['title']+train_data['tag']\n",
    "# # test_data['realtitle'] = test_data['title']+test_data['tag']\n",
    "\n",
    "# def getxx( x ):\n",
    "#     if ctrmap.get(( x['prefix'],x['realtitle'] )) is  not None:\n",
    "#         return ctrmap[( x['prefix'],x['realtitle'] )]\n",
    "#     else:\n",
    "#         return (alpha)/(alpha+beta) \n",
    "# # train_data['ph_pred_ctr'] = train_data.apply( getxx , axis =1 )\n",
    "# # test_data['ph_pred_ctr'] = test_data.apply( getxx , axis =1  )\n",
    "# # train_data['ph_pred_ctr'].to_csv('d:/predctr2.csv',index=False)\n",
    "# real_test_data['smooth_pre_title_tag_ctr'] = real_test_data.apply( getxx , axis =1  )\n",
    "# real_test_data['smooth_pre_title_tag_ctr'].to_csv('E:/competionfile/oppo/data/singleFeature/real_test_smooth_pre_title_tag_ctr.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 高级ctr 对原始ctr进行了点击率平滑过后的点击率:\n",
    "#!/usr/bin/python\n",
    "# coding=utf-8\n",
    "##这里是用贝叶斯进行点击率平滑\n",
    "# import numpy\n",
    "# import random\n",
    "# import scipy.special as special\n",
    " \n",
    "# class BayesianSmoothing(object):\n",
    "#     def __init__(self, alpha, beta):\n",
    "#         self.alpha = alpha\n",
    "#         self.beta = beta\n",
    " \n",
    "#     def sample(self, alpha, beta, num, imp_upperbound):\n",
    "#         sample = numpy.random.beta(alpha, beta, num)\n",
    "# #         print(sample)\n",
    "#         I = []\n",
    "#         C = []\n",
    "#         for clk_rt in sample:\n",
    "#             imp = imp_upperbound\n",
    "#             clk = imp * clk_rt\n",
    "#             I.append(imp)\n",
    "#             C.append(clk)\n",
    "# #         print(C)\n",
    "#         return I, C\n",
    " \n",
    "#     def update(self, imps, clks, iter_num, epsilon):\n",
    "#         for i in range(iter_num):\n",
    "#             print('cur iter'+str(i))\n",
    "#             new_alpha, new_beta = self.__fixed_point_iteration(imps, clks, self.alpha, self.beta)\n",
    "#             print('alpha {} beta{}'.format( new_alpha , new_beta ))\n",
    "#             if abs(new_alpha-self.alpha)<epsilon and abs(new_beta-self.beta)<epsilon:\n",
    "#                 break\n",
    "#             self.alpha = new_alpha\n",
    "#             self.beta = new_beta\n",
    "#             print('alpha {} beta{}'.format( self.alpha , self.beta ))\n",
    " \n",
    "#     def __fixed_point_iteration(self, imps, clks, alpha, beta):\n",
    "#         numerator_alpha = 0.0\n",
    "#         numerator_beta = 0.0\n",
    "#         denominator = 0.0\n",
    "#         for i in range(len(imps)):\n",
    "#             numerator_alpha += (special.digamma(clks[i]+alpha) - special.digamma(alpha))\n",
    "#             numerator_beta += (special.digamma(imps[i]-clks[i]+beta) - special.digamma(beta))\n",
    "#             denominator += (special.digamma(imps[i]+alpha+beta) - special.digamma(alpha+beta))\n",
    "#         return alpha*(numerator_alpha/denominator), beta*(numerator_beta/denominator)\n",
    "# train_data['realtitle'] = train_data['title']+train_data['tag']\n",
    "# kkk = train_data.groupby(['prefix','title'])\n",
    "# namelist = []\n",
    "# clicklist = []\n",
    "# countlist = []\n",
    "# for name , group in tqdm( kkk ):\n",
    "#     namelist.append(name) \n",
    "#     clicklist.append( len( group['label'][group['label'].astype(int) == 1]  ) ) \n",
    "#     countlist.append(len( group ))\n",
    "\n",
    "# bs = BayesianSmoothing(1, 1)\n",
    "# bs.update(countlist, clicklist, 1000, 0.0000000001)\n",
    "\n",
    "# ctrmap = {}\n",
    "# alpha = 0.45415064089651735 \n",
    "# beta = 0.7502334280754297\n",
    "# for i in range(len(clicklist)):\n",
    "#         ctrmap[namelist[i]] =  (clicklist[i]+alpha)/(countlist[i]+alpha+beta) \n",
    "\n",
    "##将平滑后的概率重新注入进去\n",
    "# train_data['realtitle'] = train_data['title']+train_data['tag']\n",
    "# test_data['realtitle'] = test_data['title']+test_data['tag']\n",
    "\n",
    "def getxx( x ):\n",
    "    if ctrmap.get(( x['prefix'],x['title'] )) is  not None:\n",
    "        return ctrmap[( x['prefix'],x['title'] )]\n",
    "    else:\n",
    "        return (alpha)/(alpha+beta) \n",
    "train_data['smooth_pre_title_ctr'] = train_data.apply( getxx , axis =1 )\n",
    "# test_data['ph_pred_ctr'] = test_data.apply( getxx , axis =1  )\n",
    "# train_data['ph_pred_ctr'].to_csv('d:/predctr2.csv',index=False)\n",
    "# real_test_data['smooth_pre_title_ctr'] = real_test_data.apply( getxx , axis =1  )\n",
    "# real_test_data['smooth_pre_title_ctr'].to_csv('E:/competionfile/oppo/data/singleFeature/real_test_smooth_pre_title_ctr.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##判断是否含有数字\n",
    "# contain_num\n",
    "import re\n",
    "# train_data['contain_num'] = test_data['prefix'].map( lambda x : 0 if  re.search('.*\\\\d+.*',x) else 1  )\n",
    "# test_data['contain_num'] = train_data['prefix'].map( lambda x : 0 if  re.search('.*\\\\d+.*',x) else 1  )\n",
    "# train_data['contain_num'].to_csv('E:/competionfile/oppo/data/singleFeature/contain_num.csv',index=False)\n",
    "# test_data['contain_num'].to_csv('E:/competionfile/oppo/data/singleFeature/test_contain_num.csv',index=False)\n",
    "\n",
    "real_test_data['contain_num'] = real_test_data['prefix'].map( lambda x : 0 if  re.search('.*\\\\d+.*',x) else 1  )\n",
    "real_test_data['contain_num'].to_csv('E:/competionfile/oppo/data/singleFeature/real_test_contain_num.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##判断是否含有英文\n",
    "test_data_containeng =  test_data[test_data[0].str.contains('[a-zA-Z]+')]\n",
    "train_data_containeng = train_data[train_data[0].str.contains('[a-zA-Z]+')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##是否含有特殊字符\n",
    "import re\n",
    "##可以看到有一些搜索词里面有奇奇怪怪的东西，看看有哪些奇奇怪怪的东西，找出含有特殊标点符号的搜索（这里暂时没做出来）\n",
    "valueset = set()\n",
    "for idx in train_data[train_data[1].isnull()][0].value_counts().index:\n",
    "    strre = \"[`~!@#$^&*()=|{}':;',\\\\[\\\\].<>/?~！@#￥……&*（）——|{}【】‘；：”“'。，、？]\"\n",
    "    for i in strre:\n",
    "        if i in idx:\n",
    "            valueset.add(i)\n",
    "test_data_contain_symbol =  test_data[0].map( lambda x : 1 if x in valueset else 0 )\n",
    "train_data_contain_symbol =  train_data[0].map( lambda x : 1 if x in valueset else 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 匹配搜索词和 文章的相似度（ 通过搜索词前缀 去匹配  由所有点击过该文章的搜索词的预测词概率最大的前三位预测词组成的文章 的相似度 ）\n",
    "import json\n",
    "def generateWz( item ):\n",
    "    prediction = item[1]\n",
    "    query = item[0]\n",
    "    totalstr = query + ' '\n",
    "    obj = json.loads( prediction )  \n",
    "    for kk in obj:\n",
    "        totalstr = totalstr + kk +' '\n",
    "    return totalstr\n",
    "train_data['wzcontent'] = train_data.apply( generateWz , axis = 1 )\n",
    "test_data['wzcontent'] = test_data.apply( generateWz , axis = 1 )\n",
    "\n",
    "##构造libffm所需要的dataformat\n",
    "import pandas  as pd\n",
    "\n",
    "##怎么划分训练集和测试集\n",
    "##训练集的要求 1 尽量包含完所有搜索词  先随便来试一下哦，不要管那么多\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "data = pd.read_table('E:/competionfile/oppo/new/oppo_round1_train_20180929/oppo_round1_train_20180929.txt',names=[0,1,2,3,4],header=None,encoding='utf8').astype(str)\n",
    "data = data[ data[4] != '音乐' ]\n",
    "cdata = pd.read_table('E:/competionfile/oppo/new/oppo_round1_train_20180929/oppo_round1_vali_20180929.txt',names=[0,1,2,3,4],header=None,encoding='utf8').astype(str)\n",
    "data['realname'] = data[2]+data[3]\n",
    "datagroup = data.groupby('realname')\n",
    "realdict = {}\n",
    "for name ,group in datagroup:\n",
    "    strset = set( group['wzcontent'] )\n",
    "    totalstr = ''\n",
    "    for i in strset:\n",
    "        totalstr = totalstr+i+' '\n",
    "    realdict[name]=totalstr\n",
    "data['realname'] = data[2]+data[3]\n",
    "data['query'] = data[0]\n",
    "##计算每个搜索前缀和文章内容的相关性\n",
    "ggr = data.groupby( ['query','realname'] )\n",
    "ggrset = set()\n",
    "for i , group in ggr:\n",
    "    ggrset.add( (i[0],i[1],realdict[ i[1] ]) )\n",
    "f = open('e:/ggrdicttxt.txt','w' ,encoding='utf-8')\n",
    "for i in ggrset:\n",
    "#     print(i)\n",
    "    f.writelines(   (i[0]+ ':' +i[1]+':'+i[2]+'\\n') ) \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\tkhoon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "d:\\Users\\tkhoon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "## 计算 搜索预测词第一名的相似度和所有点击过该文章的搜索预测词前两名所拼接起来的文章的相似度\n",
    "## todo 这个暂时先放在这儿一下，有点问题还解决不了，比如只出现过一次的文章，\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "train_data_click = train_data[ train_data['label'] == 1  ]\n",
    "train_data_click['realtitle'] = train_data_click['title']+train_data_click['tag']\n",
    "def predictstr2list( item ):\n",
    "    predictStr = item['query_prediction']\n",
    "    totallist = []\n",
    "    totallist.append( item['prefix'] )\n",
    "    if type( predictStr ) != str:\n",
    "        return []\n",
    "    obj = json.loads(predictStr)\n",
    "    curlist = []\n",
    "    for  i in obj:\n",
    "        curlist.append( ( i , float(obj[i]) ) )\n",
    "    curlist.sort( key = lambda x : x[1] , reverse = True )\n",
    "    listlen = len( curlist )\n",
    "    if listlen > 2:\n",
    "        listlen = 2\n",
    "    \n",
    "    for i in range(0,listlen):\n",
    "        totallist.append( curlist[i][0] ) \n",
    "    return ','.join(totallist)      \n",
    "train_data_click['predict_list'] = train_data_click.apply( predictstr2list , axis = 1 )\n",
    "\n",
    "from tqdm import tqdm\n",
    "title_group  = train_data_click.groupby('realtitle')\n",
    "realtitle_dict = {}\n",
    "for name , group in tqdm( title_group ):\n",
    "    predict_set = set()\n",
    "    for subnamem , subgroup in group.groupby('prefix'):\n",
    "        predict_set.add( subgroup.iloc[0]['predict_list'])\n",
    "    realtitle_dict[name]= ','.join( predict_set )    \n",
    "##这个暂时先放在这儿以后来做 ，因为有些问题可能还没有考虑好，\n",
    "##生成待计算列表\n",
    "def get_predict_title_titlecontent_tupe( item ):\n",
    "    return \n",
    "wait_cac_sim_list = []\n",
    "for name ,group in tqdm( train_data_click.groupby(['prefix','realtitle']) ):\n",
    "    item = group.iloc[0]\n",
    "    prefix = item['prefix']\n",
    "    predict = item['predict_list'].split(',')\n",
    "    if len( predict ) == 1:\n",
    "        predict = predict[0]\n",
    "    else:\n",
    "        predict = predict[1]\n",
    "    title = item['realtitle']\n",
    "    title_content = realtitle_dict[title]\n",
    "    wait_cac_sim_list.append(  prefix + '|:+|'+ title +  '|:+|'+predict + '|:+|' + title_content +'\\n' )\n",
    "    \n",
    "f = open( 'e:/competionfile/oppo/tempfile/predict_title_content_sim_temp.txt' ,'w',encoding='utf-8')\n",
    "for s in wait_cac_sim_list:\n",
    "    f.writelines( s )\n",
    "f .close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Users\\tkhoon\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"d:\\Users\\tkhoon\\Anaconda3\\lib\\site-packages\\tqdm\\_tqdm.py\", line 106, in run\n",
      "    if instance.miniters > 1 and \\\n",
      "AttributeError: 'tqdm' object has no attribute 'miniters'\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████| 330422/330422 [01:27<00:00, 3797.54it/s]\n"
     ]
    }
   ],
   "source": [
    "##求 前缀 - 文章 - tag 的平均点击率\n",
    "avgctr_map = {}\n",
    "for name , group in tqdm( train_data.groupby( ['prefix','title','tag'] ) ):\n",
    "    avgctr_map[name] = group['label'].mean()\n",
    "\n",
    "def getAvgCtr( item ):\n",
    "    return avgctr_map.get( ( item['prefix'],item['title'],item['tag']) , 0.37)\n",
    "train_data['prefix_title_tag_ctr'] = train_data.apply( getAvgCtr , axis=1 )\n",
    "test_data['prefix_title_tag_ctr'] = test_data.apply( getAvgCtr , axis=1 ) \n",
    "# train_data['prefix_title_tag_ctr'].to_csv('E:/competionfile/oppo/data/singleFeature/prefix_title_tag_ctr.csv',index=False)\n",
    "# test_data['prefix_title_tag_ctr'].to_csv('E:/competionfile/oppo/data/singleFeature/test_prefix_title_tag_ctr.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Users\\tkhoon\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"d:\\Users\\tkhoon\\Anaconda3\\lib\\site-packages\\tqdm\\_tqdm.py\", line 106, in run\n",
      "    if instance.miniters > 1 and \\\n",
      "AttributeError: 'tqdm' object has no attribute 'miniters'\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████| 314678/314678 [02:54<00:00, 1808.02it/s]\n"
     ]
    }
   ],
   "source": [
    "##求 前缀 - 文章 的平均点击率\n",
    "avgctr_map = {}\n",
    "for name , group in tqdm( train_data.groupby( ['prefix','title'] ) ):\n",
    "    avgctr_map[name] = group['label'].mean()\n",
    "\n",
    "def getAvgCtr( item ):\n",
    "    return avgctr_map.get( ( item['prefix'],item['title']) , 0.37)\n",
    "train_data['prefix_title_ctr'] = train_data.apply( getAvgCtr , axis=1 )\n",
    "# test_data['prefix_title_ctr'] = test_data.apply( getAvgCtr , axis=1 ) \n",
    "# train_data['prefix_title_ctr'].to_csv('E:/competionfile/oppo/data/singleFeature/prefix_title_ctr.csv',index=False)\n",
    "# test_data['prefix_title_ctr'].to_csv('E:/competionfile/oppo/data/singleFeature/test_prefix_title_ctr.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['prefix_title_tag_ctr'].to_csv('E:/competionfile/oppo/data/singleFeature/prefix_title_tag_ctr.csv',index=False)\n",
    "test_data['prefix_title_tag_ctr'].to_csv('E:/competionfile/oppo/data/singleFeature/test_prefix_title_tag_ctr.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 计算smooth_pre_title_ctr_rank\n",
    "smooth_pre_title_rank_map = {}\n",
    "# for name , group in tqdm( train_data.groupby('prefix') ):\n",
    "#     pre_title_ctr_list = []\n",
    "#     sub_map = {}\n",
    "#     for subname , subgroup in group.groupby('title'):\n",
    "#         item = subgroup.iloc[0]\n",
    "#         ctr  = item['smooth_pre_title_ctr']\n",
    "#         pre_title_ctr_list.append( ( ( name , subname ) , ctr ) )\n",
    "#     ## 进行排序\n",
    "#     pre_title_ctr_list.sort( key = lambda x : x[1] , reverse = True )\n",
    "#     ## 塞到map里面去\n",
    "#     for i in range(0,len(pre_title_ctr_list)):\n",
    "#         sub_map[ pre_title_ctr_list[i][0] ] = i\n",
    "#     smooth_pre_title_rank_map[name] = sub_map\n",
    "\n",
    "##查找每条记录自己的那个pre_titletag_ctr_rank \n",
    "def find_smooth_pre_title_ctr_rank( item ):\n",
    "    prefix = item['prefix']\n",
    "    realtitle  = item['title']\n",
    "    rank_map = smooth_pre_title_rank_map.get( prefix )\n",
    "    if rank_map is None:\n",
    "        # 先试一下为None的效果吧\n",
    "        return None\n",
    "    self_rank = rank_map.get( ( prefix , realtitle  ) , None )\n",
    "#     if self_rank is None:\n",
    "#         print( (prefix , realtitle) ) \n",
    "    return self_rank\n",
    "\n",
    "\n",
    "# train_data['pre_title_ctr_rank'] = train_data.apply( find_pre_title_ctr_rank , axis = 1 )\n",
    "# test_data['pre_title_ctr_rank'] = test_data.apply( find_pre_title_ctr_rank , axis = 1 )\n",
    "\n",
    "real_test_data['smooth_pre_title_ctr_rank'] = real_test_data.apply( find_smooth_pre_title_ctr_rank , axis = 1 )\n",
    "real_test_data['smooth_pre_title_ctr_rank'] = real_test_data['smooth_pre_title_ctr_rank']+1\n",
    "real_test_data['smooth_pre_title_ctr_rank'].to_csv('E:/competionfile/oppo/data/singleFeature/real_test_smooth_pre_title_ctr_rank.csv',index=False)\n",
    "# train_data['pre_title_ctr_rank'] = train_data['pre_title_ctr_rank']+1\n",
    "# test_data['pre_title_ctr_rank'] = test_data['pre_title_ctr_rank']+1\n",
    "# train_data['pre_title_ctr_rank'].to_csv('E:/competionfile/oppo/data/singleFeature/pre_title_ctr_rank.csv',index=False)\n",
    "# test_data['pre_title_ctr_rank'].to_csv('E:/competionfile/oppo/data/singleFeature/test_pre_title_ctr_rank.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 157082/157082 [09:36<00:00, 272.25it/s]\n"
     ]
    }
   ],
   "source": [
    "## 当前数据所对应的文章 在这个词所有推荐过的文章中的点击率排名\n",
    "pre_title_rank_map = {}\n",
    "for name , group in tqdm( train_data.groupby('prefix') ):\n",
    "    pre_title_ctr_list = []\n",
    "    sub_map = {}\n",
    "    for subname , subgroup in group.groupby('title'):\n",
    "        item = subgroup.iloc[0]\n",
    "        ctr  = item['prefix_title_ctr']\n",
    "        pre_title_ctr_list.append( ( ( name , subname ) , ctr ) )\n",
    "    ## 进行排序\n",
    "    pre_title_ctr_list.sort( key = lambda x : x[1] , reverse = True )\n",
    "    ## 塞到map里面去\n",
    "    for i in range(0,len(pre_title_ctr_list)):\n",
    "        sub_map[ pre_title_ctr_list[i][0] ] = i\n",
    "    pre_title_rank_map[name] = sub_map\n",
    "\n",
    "##查找每条记录自己的那个pre_titletag_ctr_rank \n",
    "def find_pre_title_ctr_rank( item ):\n",
    "    prefix = item['prefix']\n",
    "    realtitle  = item['title']\n",
    "    rank_map = pre_title_rank_map.get( prefix )\n",
    "    if rank_map is None:\n",
    "        # 先试一下为None的效果吧\n",
    "        return None\n",
    "    self_rank = rank_map.get( ( prefix , realtitle  ) , None )\n",
    "#     if self_rank is None:\n",
    "#         print( (prefix , realtitle) ) \n",
    "    return self_rank\n",
    "\n",
    "\n",
    "# train_data['pre_title_ctr_rank'] = train_data.apply( find_pre_title_ctr_rank , axis = 1 )\n",
    "# test_data['pre_title_ctr_rank'] = test_data.apply( find_pre_title_ctr_rank , axis = 1 )\n",
    "\n",
    "real_test_data['pre_title_ctr_rank'] = real_test_data.apply( find_pre_title_ctr_rank , axis = 1 )\n",
    "real_test_data['pre_title_ctr_rank'] = real_test_data['pre_title_ctr_rank']+1\n",
    "real_test_data['pre_title_ctr_rank'].to_csv('E:/competionfile/oppo/data/singleFeature/real_test_pre_title_ctr_rank.csv',index=False)\n",
    "# train_data['pre_title_ctr_rank'] = train_data['pre_title_ctr_rank']+1\n",
    "# test_data['pre_title_ctr_rank'] = test_data['pre_title_ctr_rank']+1\n",
    "# train_data['pre_title_ctr_rank'].to_csv('E:/competionfile/oppo/data/singleFeature/pre_title_ctr_rank.csv',index=False)\n",
    "# test_data['pre_title_ctr_rank'].to_csv('E:/competionfile/oppo/data/singleFeature/test_pre_title_ctr_rank.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 157082/157082 [04:21<00:00, 599.62it/s]\n"
     ]
    }
   ],
   "source": [
    "## 当前数据所对应的文章 在这个词所有推荐过的文章中的点击率排名\n",
    "pre_title_tag_rank_map = {}\n",
    "for name , group in tqdm( train_data.groupby('prefix') ):\n",
    "    pre_title_ctr_list = []\n",
    "    sub_map = {}\n",
    "    for subname , subgroup in group.groupby('realtitle'):\n",
    "        item = subgroup.iloc[0]\n",
    "        ctr  = item['prefix_title_tag_ctr']\n",
    "        pre_title_ctr_list.append( ( ( name , subname ) , ctr ) )\n",
    "    ## 进行排序\n",
    "    pre_title_ctr_list.sort( key = lambda x : x[1] , reverse = True )\n",
    "    ## 塞到map里面去\n",
    "    for i in range(0,len(pre_title_ctr_list)):\n",
    "        sub_map[ pre_title_ctr_list[i][0] ] = i\n",
    "    pre_title_tag_rank_map[name] = sub_map\n",
    "\n",
    "##查找每条记录自己的那个pre_titletag_ctr_rank \n",
    "def find_pre_title_ctr_rank( item ):\n",
    "    prefix = item['prefix']\n",
    "    realtitle  = item['realtitle']\n",
    "    rank_map = pre_title_tag_rank_map.get( prefix )\n",
    "    if rank_map is None:\n",
    "        # 先试一下为None的效果吧\n",
    "        return None\n",
    "    self_rank = rank_map.get( ( prefix , realtitle  ) , None )\n",
    "#     if self_rank is None:\n",
    "#         print( (prefix , realtitle) ) \n",
    "    return self_rank\n",
    "\n",
    "\n",
    "# train_data['pre_title_ctr_rank'] = train_data.apply( find_pre_title_ctr_rank , axis = 1 )\n",
    "# test_data['pre_title_ctr_rank'] = test_data.apply( find_pre_title_ctr_rank , axis = 1 )\n",
    "\n",
    "real_test_data['pre_title_tag_ctr_rank'] = real_test_data.apply( find_pre_title_ctr_rank , axis = 1 )\n",
    "real_test_data['pre_title_tag_ctr_rank'] = real_test_data['pre_title_ctr_rank']+1\n",
    "real_test_data['pre_title_tag_ctr_rank'].to_csv('E:/competionfile/oppo/data/singleFeature/real_test_pre_title_tag_ctr_rank.csv',index=False)\n",
    "# train_data['pre_title_ctr_rank'] = train_data['pre_title_ctr_rank']+1\n",
    "# test_data['pre_title_ctr_rank'] = test_data['pre_title_ctr_rank']+1\n",
    "# train_data['pre_title_ctr_rank'].to_csv('E:/competionfile/oppo/data/singleFeature/pre_title_ctr_rank.csv',index=False)\n",
    "# test_data['pre_title_ctr_rank'].to_csv('E:/competionfile/oppo/data/singleFeature/test_pre_title_ctr_rank.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11668, 8)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[ test_data['pre_title_ctr_rank'].isnull() ].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['pre_titletag_ctr_rank'].to_csv('E:/competionfile/oppo/data/singleFeature/pre_titletag_ctr_rank.csv',index=False)\n",
    "test_data['pre_titletag_ctr_rank'].to_csv('E:/competionfile/oppo/data/singleFeature/test_pre_titletag_ctr_rank.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 157082/157082 [00:23<00:00, 6790.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(45175, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  train_data['realtitle'] = train_data['title']+train_data['tag']\n",
    "# 是否是新词的标记  看一下出现次数小于2词的数据有多少\n",
    "pre_count_map = {} \n",
    "gr = train_data.groupby('prefix')\n",
    "for name , group in tqdm(gr):\n",
    "    pre_count_map[name] = len(group)\n",
    "\n",
    "\n",
    "train_data['prefix_count']  =  train_data['prefix'].map(lambda x : pre_count_map.get( x , 0 ) )\n",
    "train_data[train_data['prefix_count'] <=1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title_len 文章名字长度\n",
    "train_data['title_len'] = train_data['title'].map( lambda x : len(x) )\n",
    "test_data['title_len'] = test_data['title'].map( lambda x : len(x) )\n",
    "train_data['title_len'].to_csv('E:/competionfile/oppo/data/singleFeature/title_len.csv')\n",
    "test_data['title_len'].to_csv('E:/competionfile/oppo/data/singleFeature/test_title_len.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##计算和它相似的推荐词的点击率\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 其他基础特征\n",
    "import json\n",
    "# prediction_3_avg_point 预测推荐词前三个的准确度的平均值\n",
    "def predictstr2list( predictStr ):\n",
    "    if type( predictStr ) != str or predictStr == '{}':\n",
    "        return 0\n",
    "    obj = json.loads(predictStr)\n",
    "    curlist = []\n",
    "    for  i in obj:\n",
    "        curlist.append( ( i , float(obj[i]) ) )\n",
    "    curlist.sort( key = lambda x : x[1] , reverse = True )\n",
    "    listlen = len( curlist )\n",
    "    if listlen > 3:\n",
    "        listlen = 3\n",
    "    total = 0 \n",
    "    totallen = 0\n",
    "    for i in range(0,listlen):\n",
    "        total+=curlist[i][1]\n",
    "        totallen+=1\n",
    "    return total/totallen     \n",
    "\n",
    "# train_data['prediction_3_avg_point'] = train_data['query_prediction'].map( predictstr2list  )\n",
    "# test_data['prediction_3_avg_point'] = test_data['query_prediction'].map( predictstr2list  )\n",
    "# train_data['prediction_3_avg_point'].to_csv('E:/competionfile/oppo/data/singleFeature/prediction_3_avg_point.csv',index=False)\n",
    "# test_data['prediction_3_avg_point'].to_csv('E:/competionfile/oppo/data/singleFeature/test_prediction_3_avg_point.csv',index=False)\n",
    "\n",
    "real_test_data['prediction_3_avg_point'] = real_test_data['query_prediction'].map( predictstr2list  )\n",
    "real_test_data['prediction_3_avg_point'].to_csv('E:/competionfile/oppo/data/singleFeature/real_testprediction_3_avg_point.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 其他基础特征\n",
    "import json\n",
    "# prediction_10_avg_point 预测推荐词前10个的准确度的平均值\n",
    "def predictstr2list( predictStr ):\n",
    "    if type( predictStr ) != str or predictStr == '{}':\n",
    "        return 0\n",
    "    obj = json.loads(predictStr)\n",
    "    curlist = []\n",
    "    for  i in obj:\n",
    "        curlist.append( ( i , float(obj[i]) ) )\n",
    "    curlist.sort( key = lambda x : x[1] , reverse = True )\n",
    "    listlen = len( curlist )\n",
    "    if listlen > 9:\n",
    "        listlen = 10\n",
    "    total = 0 \n",
    "    totallen = 0\n",
    "    for i in range(0,listlen):\n",
    "        total+=curlist[i][1]\n",
    "        totallen+=1\n",
    "    return total/totallen     \n",
    "\n",
    "# train_data['prediction_10_avg_point'] = train_data['query_prediction'].map( predictstr2list  )\n",
    "# test_data['prediction_10_avg_point'] = test_data['query_prediction'].map( predictstr2list  )\n",
    "# train_data['prediction_10_avg_point'].to_csv('E:/competionfile/oppo/data/singleFeature/prediction_10_avg_point.csv',index=False)\n",
    "# test_data['prediction_10_avg_point'].to_csv('E:/competionfile/oppo/data/singleFeature/test_prediction_10_avg_point.csv',index=False)\n",
    "\n",
    "real_test_data['prediction_10_avg_point'] = real_test_data['query_prediction'].map( predictstr2list  )\n",
    "real_test_data['prediction_10_avg_point'].to_csv('E:/competionfile/oppo/data/singleFeature/real_testprediction_10_avg_point.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 其他基础特征\n",
    "import json\n",
    "# prediction_1_avg_point 预测推荐词准确度最高的那个的值\n",
    "def predictstr2list( predictStr ):\n",
    "    if type( predictStr ) != str or predictStr == '{}':\n",
    "        return 0\n",
    "    obj = json.loads(predictStr)\n",
    "    curlist = []\n",
    "    for  i in obj:\n",
    "        curlist.append( ( i , float(obj[i]) ) )\n",
    "    curlist.sort( key = lambda x : x[1] , reverse = True )\n",
    "#     listlen = len( curlist )\n",
    "#     if listlen > 9:\n",
    "#         listlen = 10\n",
    "#     total = 0 \n",
    "#     totallen = 0\n",
    "#     for i in range(0,listlen):\n",
    "#         total+=curlist[i][1]\n",
    "#         totallen+=1\n",
    "    return curlist[0][1]    \n",
    "\n",
    "# train_data['prediction_1_avg_point'] = train_data['query_prediction'].map( predictstr2list  )\n",
    "# test_data['prediction_1_avg_point'] = test_data['query_prediction'].map( predictstr2list  )\n",
    "# train_data['prediction_1_avg_point'].to_csv('E:/competionfile/oppo/data/singleFeature/prediction_1_avg_point.csv',index=False)\n",
    "# test_data['prediction_1_avg_point'].to_csv('E:/competionfile/oppo/data/singleFeature/test_prediction_1_avg_point.csv',index=False)\n",
    "\n",
    "real_test_data['prediction_1_avg_point'] = real_test_data['query_prediction'].map( predictstr2list  )\n",
    "real_test_data['prediction_1_avg_point'].to_csv('E:/competionfile/oppo/data/singleFeature/real_testprediction_1_avg_point.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 判断 prediction 列表是否为空\n",
    "train_data['prediction_isnull'] = train_data['query_prediction'].map(lambda x : x == '{}')\n",
    "test_data['prediction_isnull'] = test_data['query_prediction'].map(lambda x : x == '{}')\n",
    "train_data['prediction_isnull'].to_csv('E:/competionfile/oppo/data/singleFeature/prediction_isnull.csv',index=False)\n",
    "test_data['prediction_isnull'].to_csv('E:/competionfile/oppo/data/singleFeature/test_prediction_isnull.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# ##计算词性\n",
    "from pyhanlp import *\n",
    "ct =0 \n",
    "total = real_test_data.shape[0]\n",
    "curpercent=0\n",
    "prepercent=0\n",
    "def findDCnum( item ):\n",
    "    cclist = []\n",
    "    global ct\n",
    "    global total \n",
    "    global curpercent \n",
    "    global prepercent\n",
    "    for term in HanLP.segment(item):\n",
    "        cclist.append( str(term.nature) )\n",
    "    ct+=1\n",
    "    curpercent = int( ct/total*100 )\n",
    "    if curpercent != prepercent:\n",
    "        prepercent = curpercent\n",
    "        print(curpercent)\n",
    "    return str( cclist )\n",
    "# ##对于每一个搜索词，求出他所包含的动词的数量，\n",
    "# train_data['cixing'] = train_data['prefix'].map( findDCnum )\n",
    "# test_data['cixing'] = test_data['prefix'].map( findDCnum )\n",
    "real_test_data['cixing'] = real_test_data['prefix'].map( findDCnum )\n",
    "# ##统计共有多少种词性的词\n",
    "# # data['cx']=kkk\n",
    "for qq in range(97,123):\n",
    "    i = chr(qq)\n",
    "    def findcxcs( item ):\n",
    "        global i\n",
    "        cc=0\n",
    "        reali = item.replace(\"'\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\" \",\"\").split(',')\n",
    "        for tt in reali:\n",
    "            if tt.startswith(i):\n",
    "                cc+=1\n",
    "        return cc\n",
    "#     train_data[i]=train_data['cixing'].map( findcxcs  )\n",
    "#     test_data[i]=test_data['cixing'].map( findcxcs  )\n",
    "    real_test_data[i]=real_test_data['cixing'].map( findcxcs  )\n",
    "    \n",
    "for qq in range(97,123):\n",
    "    i = chr(qq)\n",
    "    if (  i in['a','m','n','v'] ):\n",
    "#         print( '词性 {} 为1 数量 {}'.format( i , real_test_data[ real_test_data[i] >= 1 ].shape ) )\n",
    "#         train_data[i].to_csv('E:/competionfile/oppo/data/singleFeature/cixing-'+i+'.csv',index=False)\n",
    "#         test_data[i].to_csv('E:/competionfile/oppo/data/singleFeature/test_cixing-'+i+'.csv',index=False)\n",
    "        real_test_data[i].to_csv('E:/competionfile/oppo/data/singleFeature/real_test_cixing-'+i+'.csv',index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 把数据都分成新数据和老数据部分，计算新数据中 预测词曾经在老数据中出现过的那些相关联的数据的平均点击率\n",
    "prediction_map = {}\n",
    "# ## 找一下这些生词的预测词里面的项目有多少是在训练集中出现过的\n",
    "for item in tqdm( train_data.itertuples() ):\n",
    "#     print( item )\n",
    "    prediction_str = item.query_prediction\n",
    "    obj = json.loads( prediction_str )\n",
    "    if len( obj ) == 0:\n",
    "        continue\n",
    "    for pre_str in obj:\n",
    "        pre_str_list = prediction_map.get( pre_str , [] )\n",
    "        pre_str_list.append(item.Index)\n",
    "        if len( pre_str_list ) == 1:\n",
    "            prediction_map[pre_str] = pre_str_list\n",
    "def find_ocr_status( predictions ):\n",
    "    obj = json.loads( predictions )\n",
    "    if len(obj) == 0:\n",
    "        return 0\n",
    "    for prediction in obj:\n",
    "        if prediction_map.get(prediction) is not None:\n",
    "            return 1\n",
    "    return 0\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
