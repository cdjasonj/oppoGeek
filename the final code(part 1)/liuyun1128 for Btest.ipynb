{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import Levenshtein\n",
    "from gensim import matutils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "import os\n",
    "import jieba\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from operator import itemgetter\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold as skf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.metrics import f1_score\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    if mode == \"test\":\n",
    "        train = pd.read_table('/home/ccit/tkhoon/data/sdata_train.csv', header=None,\n",
    "                              names=['prefix', 'query_prediction', 'title', 'tag', 'label', '1'], quoting=3)\n",
    "        vali = pd.read_table('/home/ccit/tkhoon/data/sdata_vali.csv', header=None, names=['prefix', 'query_prediction', 'title', 'tag', 'label'],\n",
    "                             quoting=3)\n",
    "        test = pd.read_table('/home/ccit/tkhoon/data/sdata_test.csv', header=None, names=['prefix', 'query_prediction', 'title', 'tag', '1'],\n",
    "                             quoting=3)\n",
    "    else:\n",
    "        train = pd.read_table('/home/ccit/tkhoon/data/data_train.csv', header=None,\n",
    "                              names=['prefix', 'query_prediction', 'title', 'tag', 'label', '1'], quoting=3)\n",
    "        vali = pd.read_table('/home/ccit/tkhoon/data/data_vali.csv', header=None, names=['prefix', 'query_prediction', 'title', 'tag', 'label'],\n",
    "                             quoting=3)\n",
    "        test = pd.read_table('/home/ccit/tkhoon/data/data_test.csv', header=None, names=['prefix', 'query_prediction', 'title', 'tag', '1'],\n",
    "                             quoting=3)\n",
    "\n",
    "\n",
    "    train_temp = train[train['1'].notnull()]\n",
    "    test_temp = test[test['1'].notnull()]\n",
    "\n",
    "    train_index = list(train_temp.index)\n",
    "    test_index = list(test_temp.index)\n",
    "\n",
    "    train.loc[train_index, 'tag'] = train.loc[train_index, 'label']\n",
    "    train.loc[train_index, 'label'] = train.loc[train_index, '1']\n",
    "\n",
    "    test.loc[test_index, 'tag'] = test.loc[test_index, '1']\n",
    "\n",
    "    train.drop('1', axis=1, inplace=True)\n",
    "    test.drop('1', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    test['label'] = -1\n",
    "    train['flag'] = 1\n",
    "    vali['flag'] = 2\n",
    "    test['flag'] = 3\n",
    "    data = pd.concat([train, vali, test])\n",
    "    data = data.reset_index()\n",
    "    data.drop('index', axis=1, inplace=True)\n",
    "\n",
    "    columns = ['prefix', 'query_prediction', 'title', 'tag']\n",
    "    for column in columns:\n",
    "        data[column] = data[column].astype(str)\n",
    "    data.drop( data[ data[\"label\"].isnull() ].index , inplace=True )\n",
    "    data['label'] = data['label'].astype(int)\n",
    "    return data\n",
    "\n",
    "def char_process(char):\n",
    "    # 提出无效字符\n",
    "    try:\n",
    "        char =  re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+??！，。？?、~@#￥%……&*（）:]+\", \"\", char)\n",
    "        return char\n",
    "    except:\n",
    "        return char\n",
    "\n",
    "def is_prefix_contains_upper_english(data):\n",
    "    judge = data['prefix'].apply(lambda x: len(re.findall(\"[A-Z]\", x)) > 0 )\n",
    "    # data[judge]\n",
    "    data['is_prefix_contains_upper_english'] = 0\n",
    "    data.loc[judge, 'is_prefix_contains_upper_english'] = 1\n",
    "    return data\n",
    "\n",
    "def char_lowwer_process(item):\n",
    "        if (item['is_prefix_contains_upper_english'] == 1) & (item['query_prediction'] == 'nan'):\n",
    "            return str.lower(item['prefix'])\n",
    "        else :\n",
    "            return item['prefix']\n",
    "\n",
    "def title_char_lowwer_process(item):\n",
    "        if (item['is_prefix_contains_upper_english'] == 1) & (item['query_prediction'] == 'nan'):\n",
    "            return str.lower(item['title'])\n",
    "        else :\n",
    "            return item['title']\n",
    "\n",
    "def char_cleaner(char):\n",
    "    if not isinstance(char, str):\n",
    "        char = \"null\"\n",
    "    pattern = re.compile(\"[^0-9a-zA-Z\\u4E00-\\u9FA5 ]\")\n",
    "    char = re.sub(pattern, \"\", char)\n",
    "    char = char.lower()\n",
    "    return char\n",
    "\n",
    "def is_prefix_contains_upper_english(data):\n",
    "    judge = data['prefix'].apply(lambda x: len(re.findall(\"[A-Z]\", x)) > 0 )\n",
    "    # data[judge]\n",
    "    data['is_prefix_contains_upper_english'] = 0\n",
    "    data.loc[judge, 'is_prefix_contains_upper_english'] = 1\n",
    "    return data\n",
    "\n",
    "def query_process(item):\n",
    "    try:\n",
    "        item['query_prediction'] = json.loads(item['query_prediction'])\n",
    "        return item['query_prediction']\n",
    "    except:\n",
    "        return '{}'\n",
    "\n",
    "def combine_tag(item):\n",
    "    if item['tag'] == '网页':\n",
    "        return '网站'\n",
    "    else:\n",
    "        return item['tag']\n",
    "complete_prefix_map={}\n",
    "def get_complete_prefix(item):\n",
    "        prefix = item['prefix']\n",
    "        complete_prefix = complete_prefix_map.get(prefix  )\n",
    "        if complete_prefix is not None:\n",
    "            return  complete_prefix\n",
    "        query_prediction = item['query_prediction']\n",
    "\n",
    "        if query_prediction == '{}':\n",
    "            return prefix\n",
    "\n",
    "        predict_word_dict = dict()\n",
    "        prefix = str(prefix)\n",
    "\n",
    "        for query_item, query_ratio in query_prediction.items():\n",
    "            query_item_cut = jieba.lcut(query_item)\n",
    "            item_word = \"\"\n",
    "            for item in query_item_cut:\n",
    "                if prefix not in item_word:\n",
    "                    item_word += item\n",
    "                else:\n",
    "                    if item_word not in predict_word_dict.keys():\n",
    "                        predict_word_dict[item_word] = 0.0\n",
    "                    predict_word_dict[item_word] += float(query_ratio)\n",
    "\n",
    "        if not predict_word_dict:\n",
    "            return prefix\n",
    "\n",
    "        predict_word_dict = sorted(predict_word_dict.items(), key=itemgetter(1), reverse=True)\n",
    "        complete_prefix = predict_word_dict[0][0]\n",
    "        complete_prefix_map[ prefix ] = complete_prefix\n",
    "        return complete_prefix\n",
    "\n",
    "def run_process(data):\n",
    "\n",
    "    data['prefix'] = data['prefix'].apply(char_process) #提出无效字符j\n",
    "    data['title'] = data['title'].apply(char_process) #title也去掉，之后涉及到计算相似度问题\n",
    "    data  = is_prefix_contains_upper_english(data) #判断prefix是否含有大写\n",
    "    data['prefix'] = data.apply(char_lowwer_process,axis=1) #将含有大写的prefix转为小写    有转化成小写的 ,query一定为空\n",
    "    data['title'] = data.apply(title_char_lowwer_process,axis=1)#把title也转换成小写\n",
    "    data['tag'] = data.apply(combine_tag,axis=1) #合并tag\n",
    "    return data\n",
    "\n",
    "def get_prefix_query_dic(data):\n",
    "    prefix_dic = {}\n",
    "    for index,row in data.iterrows():\n",
    "        if row['query_prediction'] != 'nan' and row['prefix'] not in prefix_dic:\n",
    "            prefix_dic[row['prefix']] = row['query_prediction']\n",
    "    return prefix_dic\n",
    "\n",
    "def null_query_prediction_process(item):\n",
    "    if item['query_prediction'] == 'nan' and item['prefix'] in prefix_dic:\n",
    "        return prefix_dic[item['prefix']]\n",
    "    else:\n",
    "        return item['query_prediction']\n",
    "\n",
    "def move_useless_char(s):\n",
    "    # 提出无效字符\n",
    "    return re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+??！，。？?、~@#￥%……&*（）:]+\", \"\", s)\n",
    "\n",
    "def query_prediction_text(query_prediction):\n",
    "    if query_prediction == '{}':\n",
    "        return ['PAD'];\n",
    "    else:\n",
    "        query_word = []\n",
    "        for i in query_prediction.keys():\n",
    "            query_word.append(i)\n",
    "    #移除query_word无效字符\n",
    "    for i in range(len(query_word)):\n",
    "        query_word[i] = move_useless_char(query_word[i])\n",
    "    return query_word\n",
    "\n",
    "def query_prediction_score(query_prediction):\n",
    "    if query_prediction == '{}':\n",
    "        return np.nan\n",
    "    else:\n",
    "        query_score = []\n",
    "        for i in query_prediction.values():\n",
    "            query_score.append(float(i))\n",
    "    return query_score\n",
    "\n",
    "def get_query_list_feature(data):\n",
    "    data['query_word'] = data['query_prediction'].apply(lambda x : query_prediction_text(x))\n",
    "    data['query_score'] = data['query_prediction'].apply(lambda x: query_prediction_score(x))\n",
    "    return data\n",
    "\n",
    "def get_word_length(item):\n",
    "    word_cut = jieba.lcut(item)\n",
    "    return len(word_cut)\n",
    "\n",
    "def title_is_in_query(item):\n",
    "    if item['query_prediction'] == '{}' or item['title'] not in item['query_word']:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def prefix_is_in_title(item):\n",
    "    if item['prefix'] == 'nan' or item['title'] == 'nan' or item['prefix'] not in item['title']:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def prefix_is_network(item):\n",
    "    if 'www' in item or 'com' in item or 'http' in item:\n",
    "        return 1\n",
    "\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def title_is_network(item):\n",
    "    if 'www' in item or 'com' in item or 'http' in item:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def prefix_is_question(item):\n",
    "    if '怎么' in item or '什么' in item or '哪' in item or '多少' in item or '谁' in item or '如何' in item:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def title_is_question(item):\n",
    "    if '怎么' in item or '什么' in item or '哪' in item or '多少' in item or '谁' in item or '如何' in item:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def prefix_title_leve_dist(item):\n",
    "    try:\n",
    "        return Levenshtein.distance(item['prefix'], item['title'])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def prefix_title_leve_rate(item):\n",
    "    try:\n",
    "        return Levenshtein.ratio(item['prefix'], item['title'])\n",
    "    except:\n",
    "        return 0\n",
    "def get_word_w2v_model():\n",
    "    w2v_model_name = \"baike_26g_news_13g_novel_229g.bin\"\n",
    "    w2v_model_path = os.path.join(\"resources\", w2v_model_name)\n",
    "    w2v_model = KeyedVectors.load_word2vec_format(w2v_model_path, binary=True, unicode_errors=\"ignore\")\n",
    "    return w2v_model\n",
    "\n",
    "size = 100\n",
    "\n",
    "def char_cleaner(char):\n",
    "    if not isinstance(char, str):\n",
    "        char = \"null\"\n",
    "    pattern = re.compile(\"[^0-9a-zA-Z\\u4E00-\\u9FA5 ]\")\n",
    "    char = re.sub(pattern, \"\", char)\n",
    "    char = char.lower()\n",
    "    return char\n",
    "\n",
    "def _get_jieba_array(words):\n",
    "    words = char_cleaner(words)\n",
    "    seg_cut = jieba.lcut(words)\n",
    "\n",
    "    w2v_array = list()\n",
    "    for word in seg_cut:\n",
    "        try:\n",
    "            similar_list = word_w2v_model[word]\n",
    "            w2v_array.append(similar_list)\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    if not w2v_array:\n",
    "        w2v_array = [None] * size\n",
    "    else:\n",
    "        w2v_array = matutils.unitvec(np.array(w2v_array).mean(axis=0))\n",
    "        return w2v_array\n",
    "\n",
    "def get_query_w2v_similar(item):\n",
    "    item_dict = {}\n",
    "    query_prediction = item['query_prediction']\n",
    "    title = item['title']  # 等下再求下prefix得呢\n",
    "    prefix = item['prefix']\n",
    "    if query_prediction == '{}':\n",
    "        item_dict['prefix_max_similar'] = None\n",
    "        item_dict['prefix_mean_similar'] = None\n",
    "        item_dict['prefix_weight_similar'] = None\n",
    "        item_dict['title_max_similar'] = None\n",
    "        item_dict['title_mean_similar'] = None\n",
    "        item_dict['title_weight_similar'] = None\n",
    "        return item_dict\n",
    "\n",
    "    query_prediction = sorted(query_prediction.items(), key=itemgetter(1), reverse=True)\n",
    "    query_prediction = query_prediction[:3]\n",
    "    similar_list = []\n",
    "    weight_similar_list = []\n",
    "    title_array = _get_jieba_array(item['title'])\n",
    "    prefix_array = _get_jieba_array(item['prefix'])\n",
    "\n",
    "    for key, value in query_prediction:\n",
    "\n",
    "        query_cut_array = _get_jieba_array(key)\n",
    "        try:\n",
    "            w2v_similar = np.dot(query_cut_array, title_array)\n",
    "        except (KeyError, ZeroDivisionError, TypeError):\n",
    "            w2v_similar = np.nan\n",
    "\n",
    "        similar_list.append(w2v_similar)\n",
    "        weight_w2v_similar = w2v_similar * float(value)\n",
    "        weight_similar_list.append(weight_w2v_similar)\n",
    "\n",
    "        max_similar = np.nanmax(similar_list)\n",
    "        mean_similar = np.nanmean(similar_list)\n",
    "        weight_similar = np.nansum(weight_similar_list)\n",
    "\n",
    "        item_dict[\"title_max_similar\"] = max_similar\n",
    "        item_dict[\"title_mean_similar\"] = mean_similar\n",
    "        item_dict[\"title_weight_similar\"] = weight_similar\n",
    "\n",
    "    for key, value in query_prediction:\n",
    "\n",
    "        query_cut_array = _get_jieba_array(key)\n",
    "        try:\n",
    "            w2v_similar = np.dot(query_cut_array, prefix_array)\n",
    "        except (KeyError, ZeroDivisionError, TypeError):\n",
    "            w2v_similar = np.nan\n",
    "\n",
    "        similar_list.append(w2v_similar)\n",
    "        weight_w2v_similar = w2v_similar * float(value)\n",
    "        weight_similar_list.append(weight_w2v_similar)\n",
    "\n",
    "        max_similar = np.nanmax(similar_list)\n",
    "        mean_similar = np.nanmean(similar_list)\n",
    "        weight_similar = np.nansum(weight_similar_list)\n",
    "\n",
    "        item_dict[\"prefiix_max_similar\"] = max_similar\n",
    "        item_dict[\"prefix_mean_similar\"] = mean_similar\n",
    "        item_dict[\"prefix_weight_similar\"] = weight_similar\n",
    "\n",
    "        return item_dict\n",
    "\n",
    "def get_prefix_w2v_similar(item):\n",
    "    title = item['title']\n",
    "    prefix = item['prefix']\n",
    "    title_array = _get_jieba_array(item['title'])\n",
    "    prefix_array = _get_jieba_array(item['prefix'])\n",
    "    try:\n",
    "        w2v_similar = np.dot(prefix_array, title_array)\n",
    "    except (KeyError, ZeroDivisionError, TypeError):\n",
    "        w2v_similar = np.nan\n",
    "    return w2v_similar\n",
    "\n",
    "# prefix也要加上\n",
    "def get_query_sim_feature(data):\n",
    "    start = time.time()\n",
    "\n",
    "    data[\"item_dict\"] = data.apply(get_query_w2v_similar, axis=1)\n",
    "\n",
    "    data['prefix_title_sim'] = data.apply(get_prefix_w2v_similar, axis=1)\n",
    "\n",
    "    print(start - time.time())\n",
    "    return data\n",
    "\n",
    "def get_feature0(item):\n",
    "    try:\n",
    "        return item['item_dict']['title_max_similar']\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def get_feature1(item):\n",
    "    try:\n",
    "        return item['item_dict']['title_mean_similar']\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def get_feature2(item):\n",
    "    try:\n",
    "        return item['item_dict']['title_weight_similar']\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def get_feature3(item):\n",
    "    try:\n",
    "        return item['item_dict']['prefiix_max_similar']\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def get_feature4(item):\n",
    "    try:\n",
    "        return item['item_dict']['prefix_mean_similar']\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def get_feature5(item):\n",
    "    try:\n",
    "        return item['item_dict']['prefix_weight_similar']\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def get_feature6(item):\n",
    "    try:\n",
    "        return max(item['query_score'])\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def get_feature7(item):\n",
    "    try:\n",
    "        return min(item['query_score'])\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def get_feature8(item):\n",
    "    try:\n",
    "        return np.mean(item['query_score'])\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "mode = \"train\"\n",
    "data = get_data()\n",
    "data = run_process(data)\n",
    "prefix_dic = get_prefix_query_dic(data)\n",
    "data['query_prediction'] = data.apply(null_query_prediction_process,axis=1)\n",
    "data['query_prediction'] = data.apply(query_process,axis=1)\n",
    "data['prefix'] = data.apply(get_complete_prefix,axis=1)  #把complete_prefix也当作基础特征\n",
    "data = get_query_list_feature(data)\n",
    "print(1)\n",
    "data['prefix_word_len'] = data['prefix'].apply(lambda x: get_word_length(x))\n",
    "data['title_word_len'] = data['title'].apply(lambda x: get_word_length(x))\n",
    "data['title-prefix_word_len'] = data['title_word_len'] - data['prefix_word_len']\n",
    "data['prefix_is_question'] = data['prefix'].apply(lambda x: prefix_is_question(x))\n",
    "data['title_is_question'] = data['title'].apply(lambda x: title_is_question(x))\n",
    "data['title_is_network'] = data['title'].apply(lambda x: title_is_network(x))\n",
    "data['prefix_is_network'] = data['prefix'].apply(lambda x: prefix_is_network(x))\n",
    "data['prefix_title_leve_dist'] = data.apply(prefix_title_leve_dist, axis=1)\n",
    "data['prefix_title_leve_rate'] = data.apply(prefix_title_leve_rate, axis=1)\n",
    "print(2)\n",
    "data['query_prediction_len'] = data['query_word'].apply(lambda x: len(x))\n",
    "data['prefix_len'] = data['prefix'].apply(lambda x: len(x))\n",
    "data['title_len'] = data['title'].apply(lambda x: len(x))\n",
    "data['title-prefix_len'] = data['title_len'] - data['prefix_len']\n",
    "data['title_is_in_query'] = data.apply(title_is_in_query, axis=1)\n",
    "data['is_prefix_in_title'] = data.apply(prefix_is_in_title, axis=1)\n",
    "print('start_w2v_feature')\n",
    "\n",
    "word_w2v_model = get_word_w2v_model()\n",
    "data = get_query_sim_feature(data)\n",
    "\n",
    "data['title_max_similar'] = data.apply(get_feature0, axis=1)\n",
    "data['title_mean_similar'] = data.apply(get_feature1, axis=1)\n",
    "data['title_weight_similar'] = data.apply(get_feature2, axis=1)\n",
    "data['prefix_max_similar'] = data.apply(get_feature3, axis=1)\n",
    "data['prefix_mean_similar'] = data.apply(get_feature4, axis=1)\n",
    "data['prefix_weight_similar'] = data.apply(get_feature5, axis=1)\n",
    "data['query_score_max'] = data.apply(get_feature6, axis=1)\n",
    "data['query_score_min'] = data.apply(get_feature7, axis=1)\n",
    "data['query_score_mean'] = data.apply(get_feature8, axis=1)\n",
    "\n",
    "print('basic_feature_finish')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pingjie train ,test ,vali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "def kmeans(data):\n",
    "    from sklearn.cluster import KMeans\n",
    "    features = ['prefix','title','tag']\n",
    "    for feature in features:\n",
    "        data[feature] = LabelEncoder().fit_transform(data[feature])\n",
    "    columns = ['query_prediction', 'label', 'flag', 'is_prefix_contains_upper_english', 'item_dict','query_word','query_score']\n",
    "    data_ = data.drop(columns,axis=1)\n",
    "    kmeans = KMeans(n_clusters=25,init='k-means++',max_iter=300,verbose=1,n_jobs=-1)\n",
    "    a = kmeans.fit_predict(data_)\n",
    "    data['kmeans'] = a\n",
    "    del data_\n",
    "    return data\n",
    "\n",
    "def run_feature(data):\n",
    "    features = ['kmeans','prefix', 'title', 'tag','is_prefix_in_title','title_is_in_query'\n",
    "                ,'prefix_word_len','title_word_len','prefix_is_question','prefix_is_network','query_score_mean','title_weight_similar'\n",
    "               ,'query_prediction_len','prefix_len','title_len','prefix_weight_similar']\n",
    "    for feature in features:\n",
    "        a = data[feature].value_counts().to_dict()\n",
    "        data[feature+'_count'] = data[feature].apply(lambda x:a[x])\n",
    "        del a\n",
    "        gc.collect()\n",
    "        print(feature)\n",
    "    print('part_1_finish')\n",
    "    gc.collect()\n",
    "    \n",
    "    for  i in range(len(features)):\n",
    "        for j in range(i+1,len(features)):\n",
    "            new_feature = features[i]+'_'+features[j]\n",
    "            data[new_feature] = data[features[i]].astype(str) + '_' + data[features[j]].astype(str)\n",
    "            data[new_feature] = LabelEncoder().fit_transform(data[new_feature])\n",
    "            new_feature_count = new_feature + '_count'\n",
    "            a = data[new_feature].value_counts().to_dict()\n",
    "            data[new_feature_count] = data[new_feature].apply(lambda x : a[x])\n",
    "            gc.collect()\n",
    "            del a\n",
    "            print(i)\n",
    "    print('part2_finish')\n",
    "    gc.collect()\n",
    "    \n",
    "    pos_features = ['kmeans','title_weight_similar_bin', 'title_mean_similar_bin','title_max_similar_bin','prefix_title_sim_bin','query_prediction_len','prefix_len','title_len',\n",
    "                    'title-prefix_len','prefix_word_len','title_word_len','title-prefix_word_len','prefix_max_similar_bin','prefix_mean_similar_bin',\n",
    "                   'prefix_weight_similar_bin']\n",
    "    for feature in pos_features:\n",
    "        train = data[(data['flag'] == 1)]\n",
    "        temp = train.groupby(feature,as_index=False)['label'].agg({feature+'_click_':'sum',feature+'_count':'count'})\n",
    "        data = pd.merge(data,temp,on=feature,how='left')\n",
    "        del train\n",
    "        del temp\n",
    "        gc.collect()\n",
    "        print(feature)\n",
    "    print('part3_finish')\n",
    "\n",
    "    features = ['prefix','title','tag']\n",
    "    for feature in features:\n",
    "        data[feature] = LabelEncoder().fit_transform(data[feature])\n",
    "    return data\n",
    "\n",
    "def co_feature_del(data):\n",
    "    threshold = 0.99\n",
    "    # Absolute value correlation matrix\n",
    "    corr_matrix = data.corr().abs()\n",
    "    # corr_matrix.head()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    # upper.head()\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    print('There are %d columns to remove.' % (len(to_drop)))\n",
    "    data.drop(to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    del upper\n",
    "    del to_drop\n",
    "    del corr_matrix\n",
    "    return data\n",
    "\n",
    "data['prefix_max_similar'] = data['prefix_max_similar'].fillna(data['prefix_max_similar'].mean())\n",
    "data['prefix_title_sim_bin'] = pd.qcut(data['prefix_title_sim'],8)\n",
    "data['prefix_title_sim_bin'] = pd.factorize(data['prefix_title_sim_bin'])[0]\n",
    "\n",
    "data['title_weight_similar_bin'] = pd.qcut(data['title_weight_similar'],10)\n",
    "data['title_weight_similar_bin'] = pd.factorize(data['title_weight_similar_bin'])[0]\n",
    "\n",
    "data['title_mean_similar_bin'] = pd.qcut(data['title_mean_similar'],5)\n",
    "data['title_mean_similar_bin'] = pd.factorize(data['title_mean_similar_bin'])[0]\n",
    "\n",
    "data['title_max_similar_bin'] = pd.qcut(data['title_max_similar'],5)\n",
    "data['title_max_similar_bin']  = pd.factorize(data['title_max_similar_bin'])[0]\n",
    "\n",
    "data['prefix_weight_similar_bin'] = pd.qcut(data['prefix_weight_similar'],10)\n",
    "data['prefix_weight_similar_bin'] = pd.factorize(data['prefix_weight_similar_bin'])[0]\n",
    "\n",
    "data['prefix_mean_similar_bin'] = pd.qcut(data['prefix_mean_similar'],5)\n",
    "data['prefix_mean_similar_bin'] = pd.factorize(data['prefix_mean_similar_bin'])[0]\n",
    "\n",
    "data['prefix_max_similar_bin'] = pd.qcut(data['prefix_max_similar'],3)\n",
    "data['prefix_max_similar_bin']  = pd.factorize(data['prefix_max_similar_bin'])[0]\n",
    "print('run_kmeans')\n",
    "data = kmeans(data)\n",
    "print('run_feature')\n",
    "gc.collect()\n",
    "data = run_feature(data)\n",
    "print('run_co_feature_del')\n",
    "data = co_feature_del(data)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "def pre_data(data):\n",
    "    # 先看一下，没有加入作为特征得\n",
    "    train = data[data['flag'] == 1]\n",
    "    vali = data[data['flag'] == 2]\n",
    "    test = data[data['flag'] == 3]\n",
    "\n",
    "    train_X_data = train.drop(['query_prediction', 'label', 'flag', 'is_prefix_contains_upper_english', 'item_dict','query_word','query_score'],\n",
    "                              axis=1)\n",
    "    vali_X_data = vali.drop(['query_prediction', 'label', 'flag', 'is_prefix_contains_upper_english', 'item_dict','query_word','query_score'],\n",
    "                            axis=1)\n",
    "    test_X_data = test.drop(['query_prediction', 'label', 'flag', 'is_prefix_contains_upper_english', 'item_dict','query_word','query_score'],\n",
    "                            axis=1)\n",
    "    train_Y_data = train['label']\n",
    "    vali_label = vali['label']\n",
    "    \n",
    "    del train\n",
    "    del vali\n",
    "    del test\n",
    "    \n",
    "    return train_X_data,vali_X_data,test_X_data,train_Y_data,vali_label\n",
    "def lgb_test(train_X_data,vali_X_data,train_Y_data,vali_label):\n",
    "\n",
    "    predict = []\n",
    "    clf = lgb.LGBMClassifier(\n",
    "    boosting_type='gbdt', subsample=1, colsample_bytree=1,\n",
    "    max_depth=-1, n_estimators=10000, objective='binary',min_child_weight = 10,\n",
    "    subsample_freq=1, num_leaves=127, reg_alpha=0,reg_lambda = 1.3,\n",
    "    random_state=2018, n_jobs=-1, learning_rate=0.1)\n",
    "\n",
    "    clf.fit(train_X_data, train_Y_data,eval_set=[(train_X_data,train_Y_data),(vali_X_data,vali_label)], eval_metric='logloss',verbose = 50, early_stopping_rounds=100)\n",
    "    predict = clf.predict_proba(vali_X_data,num_iteration=clf.best_iteration_)\n",
    "    return predict,clf\n",
    "\n",
    "def find_best_thr(predict,vali_label):\n",
    "    max = 0.0\n",
    "    max_i =0.0\n",
    "    predict = pd.DataFrame(predict)\n",
    "    predict = predict[1]\n",
    "    predict = pd.DataFrame(predict)\n",
    "    for i in np.arange(0.25, 0.4500, 0.001):\n",
    "        f1 = f1_score(vali_label, predict[1].map(lambda x: 0 if x < i else 1))\n",
    "        if (f1 > max):\n",
    "            max = f1_score(vali_label, predict[1].map(lambda x: 0 if x <= i else 1))\n",
    "            max_i = i\n",
    "    print('最大f1为', max)\n",
    "    print('此时阈值为:', max_i)\n",
    "\n",
    "    return max,max_i\n",
    "\n",
    "train_X_data,vali_X_data,test_X_data,train_Y_data,vali_label = pre_data(data)\n",
    "print('run_lgb_test')\n",
    "predict,clf = lgb_test(train_X_data,vali_X_data,test_X_data,train_Y_data,vali_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
