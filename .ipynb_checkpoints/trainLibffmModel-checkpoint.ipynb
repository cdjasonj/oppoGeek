{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##构造libffm所需要的dataformat\n",
    "import pandas  as pd\n",
    "from tqdm import tqdm\n",
    "##怎么划分训练集和测试集\n",
    "##训练集的要求 1 尽量包含完所有搜索词  先随便来试一下哦，不要管那么多\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "# train_data = pd.read_table('E:/competionfile/oppo/new/oppo_round1_train_20180929/oppo_round1_train_20180929.txt',names=[0,1,2,3,4],header=None,encoding='utf8').astype(str)\n",
    "# train_data = train_data[ train_data[4] != '音乐' ]\n",
    "# vali_data  = pd.read_table('E:/competionfile/oppo/new/oppo_round1_train_20180929/oppo_round1_vali_20180929.txt',names=[0,1,2,3,4],header=None,encoding='utf8').astype(str)\n",
    "\n",
    "# train_data.columns = ['prefix','query_prediction','title','tag','label']\n",
    "# vali_data.columns = ['prefix','query_prediction','title','tag','label']\n",
    "# train_data[ 'label' ] = train_data['label'].astype(int)\n",
    "# train_data['realtitle'] = train_data['title']+train_data['tag']\n",
    "# vali_data['realtitle'] = vali_data['title']+vali_data['tag']\n",
    "\n",
    "# train_data = train_data.reset_index()\n",
    "# train_data = train_data.drop(['index'],axis=1)\n",
    "# vali_data = vali_data.reset_index()\n",
    "# vali_data = vali_data.drop(['index'],axis=1)\n",
    "\n",
    "# test_data = pd.read_table('E:/competionfile/oppo/new/oppo_round1_train_20180929/oppo_round1_test_A_20180929.txt',header=None,encoding='utf8').astype(str)\n",
    "# test_data.columns = ['prefix','query_prediction','title','tag']\n",
    "test_data['realtitle']=test_data['title']+test_data['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def get_query_prediction_no1(item):\n",
    "    predictStr = item.query_prediction\n",
    "    obj = json.loads(predictStr)\n",
    "    if len(obj) == 0:\n",
    "        return item.prefix\n",
    "    curlist = []\n",
    "    for  i in obj:\n",
    "        curlist.append( ( i , float(obj[i]) ) )\n",
    "    curlist.sort( key = lambda x : x[1] , reverse = True )\n",
    "    return curlist[0][0]\n",
    "train_data['query_prediction_no1'] = train_data.apply( lambda x: get_query_prediction_no1( x ) ,axis=1)\n",
    "test_data['query_prediction_no1'] = test_data.apply(  lambda x: get_query_prediction_no1( x ) ,axis=1 )\n",
    "vali_data['query_prediction_no1'] = vali_data.apply(  lambda x: get_query_prediction_no1( x ) ,axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##统计平滑点击率，然后进行点击率分段平滑\n",
    "train_data['realtitle'] = train_data['title']+train_data['tag']\n",
    "kkk = train_data.groupby(['prefix','realtitle'])\n",
    "namelist = []\n",
    "clicklist = []\n",
    "countlist = []\n",
    "for name , group in kkk:\n",
    "    namelist.append(name) \n",
    "    clicklist.append( len( group['label'][group['label'].astype(int) == 1]  ) ) \n",
    "    countlist.append(len( group ))\n",
    "\n",
    "# bs = BayesianSmoothing(1, 1)\n",
    "# bs.update(countlist, clicklist, 1000, 0.0000000001)\n",
    "\n",
    "ctrmap = {}\n",
    "alpha = 0.45415064089651735 \n",
    "beta = 0.7502334280754297\n",
    "for i in range(len(clicklist)):\n",
    "        ctrmap[namelist[i]] =  (clicklist[i]+alpha)/(countlist[i]+alpha+beta) \n",
    "        \n",
    "def getxx( x ):\n",
    "    if ctrmap.get(( x['prefix'],x['realtitle'] )) is  not None:\n",
    "        return ctrmap[( x['prefix'],x['realtitle'] )]\n",
    "    else:\n",
    "        return (alpha)/(alpha+beta) \n",
    "    \n",
    "train_data['smooth_pre_title_tag_ctr'] = train_data.apply( getxx , axis =1 )\n",
    "vali_data['smooth_pre_title_tag_ctr'] = vali_data.apply( getxx , axis =1  )\n",
    "test_data['smooth_pre_title_tag_ctr'] = test_data.apply( getxx , axis =1  )\n",
    "\n",
    "# test_data['smooth_pre_title_tag_ctr'].to_csv('E:/competionfile/oppo/data/singleFeature/real_test_smooth_pre_title_tag_ctr.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 256587/256587 [00:00<00:00, 1138242.66it/s]\n"
     ]
    }
   ],
   "source": [
    "##平滑prefix_tag_ctr。、\n",
    "# kkk = train_data.groupby(['prefix','tag'])\n",
    "# namelist = []\n",
    "# clicklist = []\n",
    "# countlist = []\n",
    "# for name , group in tqdm(kkk):\n",
    "#     namelist.append(name) \n",
    "#     clicklist.append( len( group['label'][group['label'].astype(int) == 1]  ) ) \n",
    "#     countlist.append(len( group ))\n",
    "\n",
    "\n",
    "# ctrmap = {}\n",
    "##alpha 0.9677036751942488 beta1.5923004424413048\n",
    "alpha = 0.9677036751942488 \n",
    "beta1 = 1.5923004424413048\n",
    "for i in tqdm(range(len(clicklist))):\n",
    "        ctrmap[namelist[i]] =  (clicklist[i]+alpha)/(countlist[i]+alpha+beta) \n",
    "        \n",
    "def getsmooth_pre_tag_ctr( x ):\n",
    "    if ctrmap.get(( x['prefix'],x['tag'] )) is  not None:\n",
    "        return ctrmap[( x['prefix'],x['tag'] )]\n",
    "    else:\n",
    "        return (alpha)/(alpha+beta) \n",
    "    \n",
    "train_data['smooth_pre_tag_ctr'] = train_data.apply( getsmooth_pre_tag_ctr , axis =1 )\n",
    "vali_data['smooth_pre_tag_ctr'] = vali_data.apply( getsmooth_pre_tag_ctr , axis =1  )\n",
    "test_data['smooth_pre_tag_ctr'] = test_data.apply( getsmooth_pre_tag_ctr , axis =1  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##获取title的ctr,进行点击率平滑，\n",
    "\n",
    "# kkk = train_data.groupby('title')\n",
    "# namelist = []\n",
    "# clicklist = []\n",
    "# countlist = []\n",
    "# for name , group in tqdm(kkk):\n",
    "#     namelist.append(name) \n",
    "#     clicklist.append( len( group['label'][group['label'].astype(int) == 1]  ) ) \n",
    "#     countlist.append(len( group ))\n",
    "# bs = BayesianSmoothing(1, 1)\n",
    "# bs.update(countlist, clicklist, 1000, 0.0000000001)\n",
    "##alpha 0.6513369599960194 beta1.0825341128596258\n",
    "alpha = 0.6513369599960194\n",
    "betal = 1.0825341128596258\n",
    "for i in range(len(clicklist)):\n",
    "        ctrmap[namelist[i]] =  (clicklist[i]+alpha)/(countlist[i]+alpha+beta) \n",
    "        \n",
    "\n",
    "def get_smooth_title_ctr( x ):\n",
    "    if ctrmap.get( x['title'] ) is  not None:\n",
    "        return ctrmap[x['title'] ]\n",
    "    else:\n",
    "        return (alpha)/(alpha+beta) \n",
    "train_data['smooth_title_ctr'] = train_data.apply( get_smooth_title_ctr , axis =1 )\n",
    "vali_data['smooth_title_ctr'] = vali_data.apply( get_smooth_title_ctr , axis =1  )\n",
    "test_data['smooth_title_ctr'] = test_data.apply( get_smooth_title_ctr , axis =1  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##计算tag的ctr\n",
    "tag_ctr_dict = train_data.groupby('tag')['label'].mean()\n",
    "train_data['tag_ctr'] = train_data['tag'].map( lambda x : tag_ctr_dict[x] )\n",
    "vali_data['tag_ctr'] = vali_data['tag'].map( lambda x : tag_ctr_dict[x] )\n",
    "test_data['tag_ctr'] = test_data['tag'].map( lambda x : tag_ctr_dict[x] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##搜索词长度\n",
    "train_data['prefix_len'] = train_data['prefix'].map( lambda x : len(x)  )\n",
    "vali_data['prefix_len'] = vali_data['prefix'].map( lambda x : len(x)  )\n",
    "test_data['prefix_len'] = test_data['prefix'].map( lambda x : len(x)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['title_len'] = train_data['title'].map( lambda x : len(x)  )\n",
    "vali_data['title_len'] = vali_data['title'].map( lambda x : len(x)  )\n",
    "test_data['title_len'] = test_data['title'].map( lambda x : len(x)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##对ctr这些特征进行离散化\n",
    "cols= ['smooth_title_ctr','smooth_pre_tag_ctr','smooth_pre_title_tag_ctr','tag_ctr']\n",
    "\n",
    "def probability_disperse( probability ):\n",
    "    probability = int(probability*100)\n",
    "    return probability//5\n",
    "for col_name in cols:\n",
    "    train_data[ col_name + '_disperse' ] = train_data[col_name].map( lambda x : probability_disperse(x)  )\n",
    "    vali_data[col_name + '_disperse'] = vali_data[col_name].map( lambda x : probability_disperse(x)  )\n",
    "    test_data[col_name + '_disperse'] = test_data[col_name].map( lambda x : probability_disperse(x)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['prefix', 'query_prediction', 'title', 'tag', 'label', 'realtitle',\n",
       "       'smooth_pre_title_tag_ctr', 'smooth_title_ctr', 'tag_ctr', 'prefix_len',\n",
       "       'title_len', 'smooth_pre_tag_ctr', 'smooth_title_ctr_disperse',\n",
       "       'smooth_pre_tag_ctr_disperse', 'smooth_pre_title_tag_ctr_disperse',\n",
       "       'tag_ctr_disperse'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_set = set()\n",
    "realtitle_set = set()\n",
    "lx_set    = set()\n",
    "# query_title_set = set()\n",
    "# prefix_tag_set = set()\n",
    "# title_set = set()\n",
    "prediction_set = set()\n",
    "prediction_realtitle_set = set()\n",
    "prediction_tag_set = set()\n",
    "def get_id_set( data_list ):\n",
    "    global query_set\n",
    "    global realtitle_set\n",
    "    global lx_set\n",
    "#     global query_title_set\n",
    "#     global prefix_tag_set\n",
    "#     global title_set\n",
    "    global prediction_set\n",
    "#     global prediction_realtitle_set\n",
    "#     global prediction_tag_set\n",
    "    for train_data in data_list:\n",
    "        query_set = query_set | set( train_data['prefix'] )\n",
    "        realtitle_set = realtitle_set | set( train_data['realtitle'] )\n",
    "        lx_set = lx_set|set( train_data['tag'] )\n",
    "#         query_title_set = query_title_set | set( train_data['prefix'] + train_data['realtitle']  )\n",
    "#         prefix_tag_set = prefix_tag_set | set( train_data['prefix'] + train_data['tag']  )\n",
    "#         title_set = title_set | set( train_data['title'] )\n",
    "        prediction_set = prediction_set | set( train_data['query_prediction_no1'] )\n",
    "#         prediction_realtitle_set = prediction_realtitle_set | set( train_data['query_prediction_no1'] + train_data['realtitle']  )\n",
    "#         prediction_tag_set = prediction_tag_set | set( train_data['query_prediction_no1'] + train_data['tag']  )\n",
    "#         train_smooth_title_ctr_disperse_cp = train_data['smooth_title_ctr_disperse'].map( lambda x : str(x) + '-smooth_title_ctr_disperse' )\n",
    "#         train_idset4 = set( train_smooth_title_ctr_disperse_cp )\n",
    "\n",
    "#         smooth_pre_tag_ctr_disperse_cp = train_data['smooth_pre_tag_ctr_disperse'].map( lambda x : str(x) + '-smooth_pre_tag_ctr_disperse' )\n",
    "#         train_idset5 = set( smooth_pre_tag_ctr_disperse_cp )\n",
    "\n",
    "#         train_smooth_pre_title_tag_ctr_disperse_cp = train_data['smooth_pre_title_tag_ctr_disperse'].map( lambda x : str(x) + '-smooth_pre_title_tag_ctr_disperse' )\n",
    "#         train_idset6 = set( train_smooth_pre_title_tag_ctr_disperse_cp )\n",
    "\n",
    "#         train_tag_ctr_disperse = train_data['tag_ctr_disperse'].map( lambda x : str(x) + '-tag_ctr_disperse' )\n",
    "#         train_idset7 = set( train_tag_ctr_disperse )\n",
    "\n",
    "#     return train_idset |train_idset2|train_idset3\n",
    "#     return train_idset |train_idset2|train_idset3|train_idset4|train_idset5|train_idset6|train_idset7\n",
    "\n",
    "get_id_set( [ train_data , vali_data ,test_data  ]) \n",
    "\n",
    "cc = 0\n",
    "def get_libffm_id( data_set ):\n",
    "    global cc\n",
    "    data_id_map = {}\n",
    "    for t in data_set:\n",
    "        data_id_map[t] = cc\n",
    "        cc+=1\n",
    "    return data_id_map\n",
    "\n",
    "query_id_map = get_libffm_id( query_set ) \n",
    "realtitle_id_map = get_libffm_id( realtitle_set)\n",
    "lx_id_map = get_libffm_id( lx_set)\n",
    "# query_title_id_map = get_libffm_id( query_title_set )\n",
    "# prefix_tag_map = get_libffm_id( prefix_tag_set )\n",
    "# titel_id_map = get_libffm_id( title_set )\n",
    "prediction_id_map = get_libffm_id( prediction_set )\n",
    "# prediction_realtitle_id_map = get_libffm_id( prediction_realtitle_set )\n",
    "# prediction_tag_id_map = get_libffm_id( prediction_tag_set )\n",
    "\n",
    "\n",
    "# train_id_set = get_id_set( train_data )\n",
    "# vali_id_set = get_id_set( vali_data )\n",
    "# test_id_set = get_id_set( test_data )\n",
    "# total_id_set = train_id_set |vali_id_set |test_id_set\n",
    "\n",
    "\n",
    "# cc = 1\n",
    "# id2libffm_id_dict = {}\n",
    "# for i in total_id_set:\n",
    "#     id2libffm_id_dict[i] = cc\n",
    "#     cc+=1\n",
    "    \n",
    "def get_libffm_id( data ):\n",
    "    data['queryrealid'] = data['prefix'].map( lambda x : query_id_map[x]  )\n",
    "    data['wzid'] = data['realtitle'].map( lambda x : realtitle_id_map[x]  )\n",
    "    data['lxid'] = data['tag'].map( lambda x : lx_id_map[x]  )\n",
    "#     data['querytitleid'] = (data['prefix']+data['realtitle']).map( lambda x : query_title_id_map[x]  )\n",
    "#     data['querytagid'] = (data['prefix']+data['tag']).map( lambda x : prefix_tag_map[x]  )\n",
    "#     data['titleid'] = data['title'].map( lambda x : titel_id_map[x]  )\n",
    "    data['predictionid'] = data['query_prediction_no1'].map( lambda x : prediction_id_map[x]  )\n",
    "#     data['prediction_tag_id'] =( data['query_prediction_no1'] +  data['tag']).map( lambda x : prediction_tag_id_map[x]  )\n",
    "#     data['prediction_realtitle_id'] =( data['query_prediction_no1'] +  data['realtitle']).map( lambda x : prediction_realtitle_id_map[x]  )\n",
    "#     data['smooth_title_ctr_disperse_id'] = data['smooth_title_ctr_disperse'].map( lambda x : id2libffm_id_dict[str(x)+'-smooth_title_ctr_disperse']  )\n",
    "#     data['smooth_pre_tag_ctr_disperse_id'] = data['smooth_pre_tag_ctr_disperse'].map( lambda x : id2libffm_id_dict[str(x)+'-smooth_pre_tag_ctr_disperse']  )\n",
    "#     data['smooth_pre_title_tag_ctr_disperse_id'] = data['smooth_pre_title_tag_ctr_disperse'].map( lambda x : id2libffm_id_dict[str(x)+'-smooth_pre_title_tag_ctr_disperse']  )\n",
    "#     data['tag_ctr_disperse_id'] = data['tag_ctr_disperse'].map( lambda x : id2libffm_id_dict[str(x)+'-tag_ctr_disperse']  )\n",
    "    \n",
    "    data['queryrealid'] = data['queryrealid'].map( lambda x : '0:'+str(x)+':'+'1' )\n",
    "    data['wzid'] = data['wzid'].map( lambda x : '0:'+str(x)+':'+'1' )\n",
    "    data['lxid'] = data['lxid'].map( lambda x : '1:'+str(x)+':'+'1' )\n",
    "#     data['querytitleid'] = data['querytitleid'].map( lambda x : '3:'+str(x)+':'+'1' )\n",
    "#     data['querytagid'] = data['querytagid'].map( lambda x : '4:'+str(x)+':'+'1' )\n",
    "#     data['titleid'] = data['titleid'].map( lambda x : '5:'+str(x)+':'+'1' )\n",
    "    data['predictionid'] = data['predictionid'].map( lambda x : '2:'+str(x)+':'+'1' )\n",
    "#     data['prediction_realtitle_id'] = data['prediction_realtitle_id'].map( lambda x : '3:'+str(x)+':'+'1' )\n",
    "#     data['prediction_tag_id'] = data['prediction_tag_id'].map( lambda x : '4:'+str(x)+':'+'1' )\n",
    "#     data['smooth_title_ctr_disperse_id'] = data['smooth_title_ctr_disperse_id'].map( lambda x : '3:'+str(x)+':'+'1' )\n",
    "#     data['smooth_pre_tag_ctr_disperse_id'] = data['smooth_pre_tag_ctr_disperse_id'].map( lambda x : '4:'+str(x)+':'+'1' )\n",
    "#     data['smooth_pre_title_tag_ctr_disperse_id'] = data['smooth_pre_title_tag_ctr_disperse_id'].map( lambda x : '5:'+str(x)+':'+'1' )\n",
    "#     data['tag_ctr_disperse_id'] = data['tag_ctr_disperse_id'].map( lambda x : '6:'+str(x)+':'+'1' )\n",
    "\n",
    "get_libffm_id( train_data )\n",
    "get_libffm_id( vali_data )\n",
    "get_libffm_id( test_data )\n",
    "\n",
    "\n",
    "mode = 'test'\n",
    "def save_libffm_file( data , fname ):\n",
    "    \n",
    "    f = open('e:/libffm/'+fname,'w')\n",
    "    for row in data.itertuples():\n",
    "    #     print(row)\n",
    "        if mode != 'test':\n",
    "            if fname != 'test_libffm.txt':\n",
    "                ss = str(row.label)  + ' '+str(row.queryrealid)+ ' '+str(row.wzid)+' '+str(row.lxid)+' '+str(row.smooth_title_ctr_disperse_id)+' ' \\\n",
    "                    +str(row.smooth_pre_tag_ctr_disperse_id)+' '+str(row.smooth_pre_title_tag_ctr_disperse_id)+' '+str(row.tag_ctr_disperse_id)+\"\\n\"\n",
    "            else:\n",
    "                ss = 'n'  + ' '+str(row.queryrealid)+ ' '+str(row.wzid)+' '+str(row.lxid)+' '+str(row.smooth_title_ctr_disperse_id)+' ' \\\n",
    "                    +str(row.smooth_pre_tag_ctr_disperse_id)+' '+str(row.smooth_pre_title_tag_ctr_disperse_id)+' '+str(row.tag_ctr_disperse_id)+\"\\n\"\n",
    "        else:\n",
    "            if fname != 'test_libffm.txt':\n",
    "#                 ss = str(row.label)  + ' '+str(row.queryrealid)+ ' '+str(row.wzid)+' '+str(row.lxid)+' '+ \\\n",
    "#                 str(row.querytitleid)+' '+str(row.querytagid)+' '+str(row.titleid)+' '+str(row.predictionid)+' '+str(row.prediction_title_id)+' '+str(row.prediction_tag_id)+\"\\n\"\n",
    "                ss = str(row.label)+ ' '+str(row.queryrealid) + ' '+str(row.wzid)+' '+str(row.lxid)+\"\\n\"\n",
    "            else:\n",
    "                ss = 'n'+ ' '+str(row.queryrealid) + ' '+str(row.wzid)+' '+str(row.lxid)+\"\\n\"\n",
    "        f.writelines(ss)\n",
    "    f.close()    \n",
    "save_libffm_file( train_data ,'train_libffm.txt' )\n",
    "save_libffm_file( vali_data ,'vali_libffm.txt' )\n",
    "save_libffm_file( test_data ,'test_libffm.txt' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_set = query_set | set( train_data['prefix'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\tkhoon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:94: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "##构造libffm所需要的dataformat\n",
    "import pandas  as pd\n",
    "\n",
    "##怎么划分训练集和测试集\n",
    "##训练集的要求 1 尽量包含完所有搜索词  先随便来试一下哦，不要管那么多\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "train_query = train_data[0].astype(str).map( lambda x : x + '-query' )\n",
    "train_idset = set( train_query )\n",
    "train_real_title= train_data['realtitle'].map( lambda x : x + '-real_title' )\n",
    "train_idset2 = set( train_real_title )\n",
    "\n",
    "train_smooth_title_ctr_disperse_cp = train_data['smooth_title_ctr_disperse'].map( lambda x : x + '-smooth_title_ctr_disperse' )\n",
    "train_idset3 = set( train_smooth_title_ctr_disperse_cp )\n",
    "\n",
    "smooth_pre_tag_ctr_disperse_cp = train_data['smooth_pre_tag_ctr_disperse'].map( lambda x : x + '-smooth_pre_tag_ctr_disperse' )\n",
    "train_idset4 = set( smooth_pre_tag_ctr_disperse_cp )\n",
    "\n",
    "train_smooth_pre_title_tag_ctr_disperse_cp = train_data['smooth_pre_title_tag_ctr_disperse'].map( lambda x : x + '-smooth_pre_title_tag_ctr_disperse' )\n",
    "train_idset5 = set( train_smooth_pre_title_tag_ctr_disperse_cp )\n",
    "\n",
    "train_tag_ctr_disperse = train_data['tag_ctr_disperse'].map( lambda x : x + '-tag_ctr_disperse' )\n",
    "train_idset6 = set( tag_ctr_disperse )\n",
    "\n",
    "train_lx= train_data[3].map(lambda x : x+'-lx')\n",
    "train_idset7 = set( train_lx )\n",
    "\n",
    "\n",
    "vali_data['queryreal'] = vali_data[0].astype(str).map( lambda x : x + '-query' )\n",
    "idset3 = set( vali_data['queryreal'] )\n",
    "vali_data['real_title'] = vali_data['realtitle'].map( lambda x : x + '-real_title' )\n",
    "idset4 = set( vali_data['real'] )\n",
    "\n",
    "train_data['lxreal']= train_data[3].map(lambda x : x+'-lx')\n",
    "vali_data['lxreal'] =  vali_data[3].map(lambda x : x+'-lx')\n",
    "\n",
    "idset5 = set( train_data['lxreal'] )\n",
    "idset6 = set( vali_data['lxreal']) \n",
    "\n",
    "###加入测试集\n",
    "test_data = pd.read_table('E:/competionfile/oppo/new/oppo_round1_train_20180929/oppo_round1_test_A_20180929.txt',names=[0,1,2,3,4],header=None,encoding='utf8').astype(str)\n",
    "test_data['queryreal'] = test_data[0].astype(str).map( lambda x : x + '-query' )\n",
    "idset7 = set( test_data['queryreal'] )\n",
    "test_data['real'] = test_data[2]+test_data[3]\n",
    "test_data['real'] = test_data['real'].map( lambda x : x + '-real' )\n",
    "idset8 = set( test_data['real'] )\n",
    "test_data['lxreal']= test_data[3].map(lambda x : x+'-lx')\n",
    "idset9 = set( test_data['lxreal']) \n",
    "\n",
    "\n",
    "\n",
    "# idset = idset | idset2 | idset3 | idset4 | idset5 | idset6 | idset7 | idset8 | idset9\n",
    "# cc = 1\n",
    "# ccdict = {}\n",
    "# for i in idset:\n",
    "#     ccdict[i] = cc\n",
    "#     cc+=1\n",
    "# data['queryrealid'] = data['queryreal'].map( lambda x : ccdict[x]  )\n",
    "# data['wzid'] = data['real'].map( lambda x : ccdict[x]  )\n",
    "# data['lxid'] = data['lxreal'].map( lambda x : ccdict[x]  )\n",
    "# cdata['queryrealid'] = cdata['queryreal'].map( lambda x : ccdict[x]  )\n",
    "# cdata['wzid'] = cdata['real'].map( lambda x : ccdict[x]  )\n",
    "# cdata['lxid'] = cdata['lxreal'].map( lambda x : ccdict[x]  )\n",
    "# test_data['queryrealid'] = test_data['queryreal'].map( lambda x : ccdict[x]  )\n",
    "# test_data['wzid'] = test_data['real'].map( lambda x : ccdict[x]  )\n",
    "# test_data['lxid'] = test_data['lxreal'].map( lambda x : ccdict[x]  )\n",
    "\n",
    "# data['queryrealid'] = data['queryrealid'].map( lambda x : '0:'+str(x)+':'+'1' )\n",
    "# data['wzid'] = data['wzid'].map( lambda x : '1:'+str(x)+':'+'1' )\n",
    "# data['lxid'] = data['lxid'].map( lambda x : '2:'+str(x)+':'+'1' )\n",
    "# cdata['queryrealid'] = cdata['queryrealid'].map( lambda x : '0:'+str(x)+':'+'1' )\n",
    "# cdata['wzid'] = cdata['wzid'].map( lambda x : '1:'+str(x)+':'+'1' )\n",
    "# cdata['lxid'] = cdata['lxid'].map( lambda x : '2:'+str(x)+':'+'1' )\n",
    "# test_data['queryrealid'] = test_data['queryrealid'].map( lambda x : '0:'+str(x)+':'+'1' )\n",
    "# test_data['wzid'] = test_data['wzid'].map( lambda x : '1:'+str(x)+':'+'1' )\n",
    "# test_data['lxid'] = test_data['lxid'].map( lambda x : '2:'+str(x)+':'+'1' )\n",
    "\n",
    "# f = open('e:/libffm/train2.txt','w')\n",
    "# # for  i  in \n",
    "# X_train = data[[4,'queryrealid','wzid','lxid']]\n",
    "# X_train['rate'] = data[4]\n",
    "# for row in X_train.itertuples():\n",
    "# #     print(row)\n",
    "#     ss = str(row.rate)  + ' '+str(row.queryrealid)+ ' '+str(row.wzid)+' '+str(row.lxid)+\"\\n\"\n",
    "#     f.writelines(ss)\n",
    "# #     print>>f\n",
    "# f.close()\n",
    "# f = open('e:/libffm/valid2.txt','w')\n",
    "# # for  i  in \n",
    "# X_train = cdata[[4,'queryrealid','wzid','lxid']]\n",
    "# X_train['rate'] = cdata[4]\n",
    "# for row in X_train.itertuples():\n",
    "# #     print(row)\n",
    "#     ss = str(row.rate)  + ' '+str(row.queryrealid)+ ' '+str(row.wzid)+' '+str(row.lxid)+\"\\n\"\n",
    "#     f.writelines(ss)\n",
    "# #     print>>f\n",
    "# f.close()\n",
    "\n",
    "f = open('e:/libffm/test2.txt','w')\n",
    "# for  i  in \n",
    "X_train = test_data[[4,'queryrealid','wzid','lxid']]\n",
    "X_train['rate'] = test_data[4]\n",
    "for row in X_train.itertuples():\n",
    "#     print(row)\n",
    "    ss = str(row.rate)  + ' '+str(row.queryrealid)+ ' '+str(row.wzid)+' '+str(row.lxid)+\"\\n\"\n",
    "    f.writelines(ss)\n",
    "#     print>>f\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 点击率平滑， 离散化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0  0.81846005 0.85978036 0.83861153     31415\n",
      "          1  0.74086711 0.67764326 0.70784622     18585\n",
      "\n",
      "avg / total  0.78961875 0.79208000 0.79000606     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##现在来校验一下ffm的效果\n",
    "import pandas as pd \n",
    "##得到结果\n",
    "# pred = pd.read_table('E:libffm/result792.txt',header=None)[0].map(lambda x : 1 if x >= 0.5 else 0)\n",
    "pred = pd.read_table('E:/libffm/validresult.txt',header=None)[0].map(lambda x : 1 if x >= 0.5 else 0)\n",
    "da = pd.read_table('E:/useritemratetrainvalid.txt',header=None,delimiter =' ')[2]\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(da, pred  ,digits=8)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred\n",
    "pred = pd.read_table('E:/libffm/test_result.txt',header=None)[0].map(lambda x : 1 if x >= 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.to_csv('E:/libffm/test_result.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pyhanlp import *\n",
    "def qpj( item ):\n",
    "#     print(item)\n",
    "    itemstr = item[1]\n",
    "    if type( itemstr ) != str:\n",
    "        return 0\n",
    "    obj = json.loads(itemstr)\n",
    "    curlist = []\n",
    "#     print(type(obj))\n",
    "#     print(obj)\n",
    "    totalstr = item[0]+','\n",
    "    for  i in obj:\n",
    "        totalstr = totalstr + i + ','\n",
    "    kkl = HanLP.extractKeyword(totalstr, 3);\n",
    "    return kkl\n",
    "# data['kkl'] = data.apply( lambda x : qpj(x) , axis=1 )\n",
    "# cdata['kkl'] = cdata.apply( lambda x : qpj(x) , axis=1 )\n",
    "# data['queryreal'] = data[0].astype(str).map( lambda x : x+'-search' )\n",
    "# idset0 = set( data['queryreal'] )\n",
    "# cdata['queryreal'] = cdata[0].astype(str).map( lambda x : x+'-search' )\n",
    "# idset11 = set( cdata['queryreal'] )\n",
    "data['kkl1'] = data['kkl'].map( lambda x : x[0] if len(x) >= 1 else ' ' )\n",
    "data['kkl2'] = data['kkl'].map( lambda x : x[1] if len(x) >=2 else ' ' )\n",
    "data['kkl3'] = data['kkl'].map( lambda x : x[2] if len(x) >=3 else ' ')\n",
    "\n",
    "cdata['kkl1'] = cdata['kkl'].map( lambda x : x[0] if len(x) >= 1 else ' ' )\n",
    "cdata['kkl2'] = cdata['kkl'].map( lambda x : x[1] if len(x) >=2 else ' ' )\n",
    "cdata['kkl3'] = cdata['kkl'].map( lambda x : x[2] if len(x) >=3 else ' ')\n",
    "##编号\n",
    "\n",
    "##构造libffm所需要的dataformat\n",
    "import pandas  as pd\n",
    "\n",
    "##怎么划分训练集和测试集\n",
    "##训练集的要求 1 尽量包含完所有搜索词  先随便来试一下哦，不要管那么多\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# data = pd.read_table('E:/competionfile/oppo/new/oppo_round1_train_20180929/oppo_round1_vali_20180929.txt',names=[0,1,2,3,4],header=None,encoding='utf8').astype(str)\n",
    "# data = data[ data[4] != '音乐' ]\n",
    "data['kkl1real'] = data['kkl1'].astype(str).map( lambda x : x + '-query' )\n",
    "idset = set( data['kkl1real'] )\n",
    "data['kkl2real'] = data['kkl2'].astype(str).map( lambda x : x + '-query' )\n",
    "idset2 = set( data['kkl2real'] )\n",
    "data['kkl3real'] = data['kkl3'].astype(str).map( lambda x : x + '-query' )\n",
    "idset3 = set( data['kkl3real'] )\n",
    "# data['real'] = data[2]+data[3]\n",
    "# data['real'] = data['real'].map( lambda x : x + '-real' )\n",
    "# idset4 = set( data['real'] )\n",
    "\n",
    "\n",
    "cdata['kkl1real'] = cdata['kkl1'].astype(str).map( lambda x : x + '-query' )\n",
    "idset5 = set( cdata['kkl1real'] )\n",
    "cdata['kkl2real'] = cdata['kkl2'].astype(str).map( lambda x : x + '-query' )\n",
    "idset6 = set( cdata['kkl2real'] )\n",
    "cdata['kkl3real'] = cdata['kkl3'].astype(str).map( lambda x : x + '-query' )\n",
    "idset7 = set( cdata['kkl3real'] )\n",
    "cdata['real'] = cdata[2]+data[3]\n",
    "cdata['real'] = cdata['real'].astype(str).map( lambda x : x + '-real' )\n",
    "idset8 = set( cdata['real'] )\n",
    "\n",
    "data['lxreal']= data[3].map(lambda x : x+'-lx')\n",
    "cdata['lxreal'] =  cdata[3].map(lambda x : x+'-lx')\n",
    "\n",
    "idset9 = set( data['lxreal'] )\n",
    "idset10 = set( cdata['lxreal']) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "idset = idset0|idset | idset2 | idset3 | idset4 | idset5 | idset6 | idset7 | idset8 | idset9 | idset10 | idset11\n",
    "cc = 1\n",
    "ccdict = {}\n",
    "for i in idset:\n",
    "    ccdict[i] = cc\n",
    "    cc+=1\n",
    "data['kkl1'] = data['kkl1real'].map( lambda x : ccdict[x]  )\n",
    "data['kkl2'] = data['kkl2real'].map( lambda x : ccdict[x]  )\n",
    "data['kkl3'] = data['kkl3real'].map( lambda x : ccdict[x]  )\n",
    "data['wzid'] = data['real'].map( lambda x : ccdict[x]  )\n",
    "data['lxid'] = data['lxreal'].map( lambda x : ccdict[x]  )\n",
    "data['queryid'] = data['queryreal'].map( lambda x : ccdict[x]  )\n",
    "\n",
    "cdata['kkl1'] = cdata['kkl1real'].map( lambda x : ccdict[x]  )\n",
    "cdata['kkl2'] = cdata['kkl2real'].map( lambda x : ccdict[x]  )\n",
    "cdata['kkl3'] = cdata['kkl3real'].map( lambda x : ccdict[x]  )\n",
    "cdata['wzid'] = cdata['real'].map( lambda x : ccdict[x]  )\n",
    "cdata['lxid'] = cdata['lxreal'].map( lambda x : ccdict[x]  )\n",
    "cdata['queryid'] = cdata['queryreal'].map( lambda x : ccdict[x]  )\n",
    "\n",
    "data['kkl1'] = data['kkl1'].map( lambda x : '0:'+str(x)+':'+'1' )\n",
    "data['kkl2'] = data['kkl2'].map( lambda x : '1:'+str(x)+':'+'1' )\n",
    "data['kkl3'] = data['kkl3'].map( lambda x : '2:'+str(x)+':'+'1' )\n",
    "data['wzid'] = data['wzid'].map( lambda x : '3:'+str(x)+':'+'1' )\n",
    "data['lxid'] = data['lxid'].map( lambda x : '4:'+str(x)+':'+'1' )\n",
    "data['queryid'] = data['queryid'].map( lambda x : '5:'+str(x)+':'+'1' )\n",
    "cdata['kkl1'] = cdata['kkl1'].map( lambda x : '0:'+str(x)+':'+'1' )\n",
    "cdata['kkl2'] = cdata['kkl2'].map( lambda x : '1:'+str(x)+':'+'1' )\n",
    "cdata['kkl3'] = cdata['kkl3'].map( lambda x : '2:'+str(x)+':'+'1' )\n",
    "cdata['wzid'] = cdata['wzid'].map( lambda x : '3:'+str(x)+':'+'1' )\n",
    "cdata['lxid'] = cdata['lxid'].map( lambda x : '4:'+str(x)+':'+'1' )\n",
    "cdata['queryid'] = cdata['queryid'].map( lambda x : '5:'+str(x)+':'+'1' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
