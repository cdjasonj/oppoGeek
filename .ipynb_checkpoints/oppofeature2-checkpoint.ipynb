{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##构造libffm所需要的dataformat\n",
    "import pandas  as pd\n",
    "from tqdm import tqdm\n",
    "##怎么划分训练集和测试集\n",
    "##训练集的要求 1 尽量包含完所有搜索词  先随便来试一下哦，不要管那么多\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "train_data = pd.read_table('E:/competionfile/oppo/new/oppo_round1_train_20180929/oppo_round1_train_20180929.txt',names=[0,1,2,3,4],header=None,encoding='utf8').astype(str)\n",
    "train_data = train_data[ train_data[4] != '音乐' ]\n",
    "# test_data  = pd.read_table('E:/competionfile/oppo/new/oppo_round1_train_20180929/oppo_round1_vali_20180929.txt',names=[0,1,2,3,4],header=None,encoding='utf8').astype(str)\n",
    "# train_data = train_data[ train_data[3] != '推广' ] \n",
    "# test_data =  test_data[ test_data[3] != '推广' ] \n",
    "\n",
    "# train_data.columns = ['prefix','query_prediction','title','tag','label']\n",
    "# test_data.columns = ['prefix','query_prediction','title','tag','label']\n",
    "# train_data[ 'label' ] = train_data['label'].astype(int)\n",
    "# train_data['realtitle'] = train_data['title']+train_data['tag']\n",
    "# test_data['realtitle'] = test_data['title']+test_data['tag']\n",
    "\n",
    "# train_data = train_data.reset_index()\n",
    "# train_data = train_data.drop(['index'],axis=1)\n",
    "# test_data = test_data.reset_index()\n",
    "# test_data = test_data.drop(['index'],axis=1)\n",
    "\n",
    "# real_test_data = pd.read_table('E:/competionfile/oppo/new/oppo_round1_train_20180929/oppo_round1_test_A_20180929.txt',header=None,encoding='utf8').astype(str)\n",
    "# real_test_data.columns = ['prefix','query_prediction','title','tag']\n",
    "# real_test_data['realtitle']=real_test_data['title']+real_test_data['tag']\n",
    "# train_data['realtitle'] = train_data['title']+train_data['tag']\n",
    "\n",
    "train_data['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\tkhoon\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:6201: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'querydict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-1785fed2911c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mwaitsimlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mquerytitlegroup\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mwaitsimlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mquerydict\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m##将waitsimlis分成4分\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'querydict' is not defined"
     ]
    }
   ],
   "source": [
    "##计算搜索预测词中最大的前两个搜索词和文章名字的相似度  ( 这个特征重新弄一下， 不用百度的familar 来做了，换成词向量和句向量的方式 )\n",
    "totaldata = train_data.append( test_data , ignore_index = True  )\n",
    "totaldata = totaldata.append( real_test_data , ignore_index = True  )\n",
    "\n",
    "totaldata['query']=totaldata['prefix']\n",
    "totaldata['prediction'] = totaldata['query_prediction']\n",
    "totaldata['realname'] = totaldata['title']+totaldata['tag']\n",
    "querytitlegroup = totaldata.groupby( ['query','realname'] )\n",
    "waitsimlist = []\n",
    "for name ,group in querytitlegroup:\n",
    "    waitsimlist.append( ( name , querydict[ name[0] ]) )\n",
    "    \n",
    "##将waitsimlis分成4分\n",
    "print(int(len(waitsimlist)/4))\n",
    "waitsimlist1 = waitsimlist[0:int(len(waitsimlist)/4)]\n",
    "waitsimlist2 = waitsimlist[int(len(waitsimlist)/4):int(len(waitsimlist)/2)]\n",
    "waitsimlist3 = waitsimlist[int(len(waitsimlist)/2):int(len(waitsimlist)/4*3)]\n",
    "waitsimlist4 = waitsimlist[int(len(waitsimlist)/4*3):len(waitsimlist)]\n",
    "\n",
    "f = open( 'e:/waitsimlist1.txt','w',encoding='utf-8')\n",
    "for item in waitsimlist:\n",
    "    f.writelines(  item[0][0] +'&&&&&'+item[0][1]+'&&&&&'+item[1]+'\\n'  )\n",
    "f.close()\n",
    "# f = open( 'e:/waitsimlist2.txt','w',encoding='utf-8')\n",
    "# for item in waitsimlist2:\n",
    "#     f.writelines(  item[0][0] +'&&&&&'+item[0][1]+'&&&&&'+item[1]+'\\n'  )\n",
    "# f.close()\n",
    "# f = open( 'e:/waitsimlist3.txt','w',encoding='utf-8')\n",
    "# for item in waitsimlist3:\n",
    "#     f.writelines(  item[0][0] +'&&&&&'+item[0][1]+'&&&&&'+item[1]+'\\n'  )\n",
    "# f.close()\n",
    "# f = open( 'e:/waitsimlist4.txt','w',encoding='utf-8')\n",
    "# for item in waitsimlist4:\n",
    "#     f.writelines(  item[0][0] +'&&&&&'+item[0][1]+'&&&&&'+item[1]+'\\n'  )\n",
    "f.close()\n",
    "\n",
    "##放入familiar文件中进行预测\n",
    "\n",
    "##文件路径 C:\\Users\\tkhoon\\Documents\\oppo比赛\\query_doc_sim_demo.py\n",
    "\n",
    "\n",
    "##将预测出来的相似度的值填入到原来的data中\n",
    "# import pickle \n",
    "# import sys\n",
    "# # reload(sys)\n",
    "# # sys.setdefaultencoding( 'utf-8' )\n",
    "# def ccacdis( cdismap , prefix ):\n",
    "#     dismap = cdismap\n",
    "#     f= open('e:/result/'+prefix+'result1.pickle','rb')\n",
    "#     resultlist = pickle.load(f)\n",
    "#     for row in resultlist:\n",
    "#     #     print(row[0])\n",
    "#         dismap[ (row[0],row[1]) ]=row[2]\n",
    "\n",
    "#     f= open('e:/result/'+prefix+'result2.pickle','rb')\n",
    "#     resultlist = pickle.load(f)\n",
    "#     for row in resultlist:\n",
    "#     #     print(row[0])\n",
    "#         dismap[ (row[0],row[1]) ]=row[2]\n",
    "#     f= open('e:/result/'+prefix+'result3.pickle','rb')\n",
    "#     resultlist = pickle.load(f)\n",
    "#     for row in resultlist:\n",
    "#     #     print(row[0])\n",
    "#         dismap[ (row[0],row[1]) ]=row[2]\n",
    "\n",
    "#     f= open('e:/result/'+prefix+'result4.pickle','rb')\n",
    "#     resultlist = pickle.load(f)\n",
    "#     for row in resultlist:\n",
    "#     #     print(row[0])\n",
    "#         dismap[ (row[0],row[1]) ]=row[2]\n",
    "        \n",
    "\n",
    "# dismap={}\n",
    "# newsdismap={}\n",
    "# weibodismap={}\n",
    "# ccacdis(dismap,'')\n",
    "# ccacdis(newsdismap,'news')\n",
    "# ccacdis(weibodismap,'weibo')\n",
    "\n",
    "# data['query'] = data[0]\n",
    "# data['realname'] = data[2]+data[3]\n",
    "# cdata['query'] = cdata[0]\n",
    "# cdata['realname'] = cdata[2]+cdata[3]\n",
    "# def getDis( row ):\n",
    "#     if dismap.get(( row['query'] , row['realname'] )) is not None:\n",
    "#         return (dismap.get(( row['query'] , row['realname'] ))+newsdismap.get(( row['query'] , row['realname'] ))+weibodismap.get(( row['query'] , row['realname'] )))/3\n",
    "#     else:\n",
    "#         return 0\n",
    "# data['avgpredictwzdis']= data.apply( getDis ,axis = 1 )\n",
    "# cdata['avgpredictwzdis'] = cdata.apply( getDis ,axis = 1 )\n",
    "# data['avgpredictwzdis'].to_csv( 'e:/avgpredictwzdis.csv', index=False )\n",
    "# cdata['avgpredictwzdis'].to_csv( 'e:/cavgcpredictwzdis.csv', index=False )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# namemap.get('猴儿酒百科')\n",
    "# for name ,group in tqdm(zzk):\n",
    "#     nameset = set( group['prefix'].value_counts().index )\n",
    "#     namemap[ name ] = nameset\n",
    "#     if name == '猴儿酒百科':\n",
    "#         print(name)\n",
    "#     print(name)\n",
    "# nameset = set( zzk )\n",
    "\n",
    "\n",
    "# totaldata[ totaldata['realtitle'] == '猴儿酒百科' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 135350/135350 [03:45<00:00, 601.05it/s]\n"
     ]
    }
   ],
   "source": [
    "##kk( 计算当前数据中预测词中概率最高的那一个和曾今点过该文章的所有搜索词中相似度最高的搜索词的相似度 )\n",
    "##原名叫kk 现在改名为 pre_titile_pre_difsim \n",
    "totaldata = train_data[ train_data['label'] == 1  ]\n",
    "zzk=totaldata.groupby('realtitle')\n",
    "namemap={}\n",
    "for name ,group in tqdm(zzk):\n",
    "    nameset = set( group['prefix'].value_counts().index )\n",
    "    namemap[ name ] = nameset\n",
    "\n",
    "\n",
    "import difflib\n",
    "import Levenshtein\n",
    "def latt( s1 , s2 ):\n",
    "    return Levenshtein.distance(s1,s2)\n",
    "def get_equal_rate_1(str1,str2):\n",
    "    return difflib.SequenceMatcher(None, str1, str2).quick_ratio()\n",
    "\n",
    "def findmax( item ):\n",
    "    query = item['prefix']\n",
    "    wzname = item['realtitle']\n",
    "    maxrate = 0\n",
    "    item_prefix_map = namemap.get(wzname)\n",
    "    if item_prefix_map is None:\n",
    "        return None\n",
    "    for i in item_prefix_map:\n",
    "        if query !=i:\n",
    "            currate = get_equal_rate_1( query , i )\n",
    "            if currate > maxrate:\n",
    "                maxrate = currate\n",
    "    if maxrate == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return maxrate\n",
    "# pre_titile_pre_difsim = train_data.apply( findmax ,axis=1)\n",
    "# test_pre_titile_pre_difsim = test_data.apply( findmax ,axis=1)\n",
    "# pre_titile_pre_difsim.to_csv('E:/competionfile/oppo/data/singleFeature/pre_titile_pre_difsim.csv',index=False)\n",
    "# test_pre_titile_pre_difsim.to_csv('E:/competionfile/oppo/data/singleFeature/test_pre_titile_pre_difsim.csv',index=False)\n",
    "# realtest_pre_titile_pre_difsim = real_test_data.apply( findmax ,axis=1)\n",
    "# realtest_pre_titile_pre_difsim.to_csv('E:/competionfile/oppo/data/singleFeature/real_test_pre_titile_pre_difsim.csv',index=False)\n",
    "# ##原名叫ykk 现在改名为 pre_titile_pre_latsim \n",
    "\n",
    "def findmin( item ):\n",
    "    query = item['prefix']\n",
    "    wzname = item['realtitle']\n",
    "    minrate = 30\n",
    "    find = False\n",
    "    item_prefix_map = namemap.get(wzname)\n",
    "    if item_prefix_map is None:\n",
    "        return None\n",
    "    for i in item_prefix_map:\n",
    "        if query !=i:\n",
    "            find =True\n",
    "            currate = latt( query , i )\n",
    "            if currate < minrate:\n",
    "                minrate = currate\n",
    "    if not find:\n",
    "        return None\n",
    "    else:\n",
    "        return minrate\n",
    "# pre_titile_pre_latsim = train_data.apply( findmin ,axis=1)\n",
    "# test_pre_titile_pre_latsim = test_data.apply( findmin ,axis=1)\n",
    "# pre_titile_pre_latsim.to_csv('E:/competionfile/oppo/data/singleFeature/pre_titile_pre_latsim.csv',index=False)\n",
    "# test_pre_titile_pre_latsim.to_csv('E:/competionfile/oppo/data/singleFeature/test_pre_titile_pre_latsim.csv',index=False)\n",
    "real_test_pre_titile_pre_latsim = real_test_data.apply( findmin ,axis=1)\n",
    "real_test_pre_titile_pre_latsim.to_csv('E:/competionfile/oppo/data/singleFeature/real_test_pre_titile_pre_latsim.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1044182, 6)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[ pre_titile_pre_difsim.isnull()  ].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 高级ctr 对原始ctr进行了点击率平滑过后的点击率:\n",
    "#!/usr/bin/python\n",
    "# coding=utf-8\n",
    "##这里是用贝叶斯进行点击率平滑\n",
    "# import numpy\n",
    "# import random\n",
    "# import scipy.special as special\n",
    " \n",
    "# class BayesianSmoothing(object):\n",
    "#     def __init__(self, alpha, beta):\n",
    "#         self.alpha = alpha\n",
    "#         self.beta = beta\n",
    " \n",
    "#     def sample(self, alpha, beta, num, imp_upperbound):\n",
    "#         sample = numpy.random.beta(alpha, beta, num)\n",
    "# #         print(sample)\n",
    "#         I = []\n",
    "#         C = []\n",
    "#         for clk_rt in sample:\n",
    "#             imp = imp_upperbound\n",
    "#             clk = imp * clk_rt\n",
    "#             I.append(imp)\n",
    "#             C.append(clk)\n",
    "# #         print(C)\n",
    "#         return I, C\n",
    " \n",
    "#     def update(self, imps, clks, iter_num, epsilon):\n",
    "#         for i in range(iter_num):\n",
    "#             print('cur iter'+str(i))\n",
    "#             new_alpha, new_beta = self.__fixed_point_iteration(imps, clks, self.alpha, self.beta)\n",
    "#             print('alpha {} beta{}'.format( new_alpha , new_beta ))\n",
    "#             if abs(new_alpha-self.alpha)<epsilon and abs(new_beta-self.beta)<epsilon:\n",
    "#                 break\n",
    "#             self.alpha = new_alpha\n",
    "#             self.beta = new_beta\n",
    "#             print('alpha {} beta{}'.format( self.alpha , self.beta ))\n",
    " \n",
    "#     def __fixed_point_iteration(self, imps, clks, alpha, beta):\n",
    "#         numerator_alpha = 0.0\n",
    "#         numerator_beta = 0.0\n",
    "#         denominator = 0.0\n",
    "#         for i in range(len(imps)):\n",
    "#             numerator_alpha += (special.digamma(clks[i]+alpha) - special.digamma(alpha))\n",
    "#             numerator_beta += (special.digamma(imps[i]-clks[i]+beta) - special.digamma(beta))\n",
    "#             denominator += (special.digamma(imps[i]+alpha+beta) - special.digamma(alpha+beta))\n",
    "#         return alpha*(numerator_alpha/denominator), beta*(numerator_beta/denominator)\n",
    "train_data['realtitle'] = train_data['title']+train_data['tag']\n",
    "kkk = train_data.groupby(['prefix','realtitle'])\n",
    "namelist = []\n",
    "clicklist = []\n",
    "countlist = []\n",
    "for name , group in kkk:\n",
    "    namelist.append(name) \n",
    "    clicklist.append( len( group['label'][group['label'].astype(int) == 1]  ) ) \n",
    "    countlist.append(len( group ))\n",
    "\n",
    "# bs = BayesianSmoothing(1, 1)\n",
    "# bs.update(countlist, clicklist, 1000, 0.0000000001)\n",
    "\n",
    "ctrmap = {}\n",
    "alpha = 0.45415064089651735 \n",
    "beta = 0.7502334280754297\n",
    "for i in range(len(clicklist)):\n",
    "        ctrmap[namelist[i]] =  (clicklist[i]+alpha)/(countlist[i]+alpha+beta) \n",
    "\n",
    "##将平滑后的概率重新注入进去\n",
    "# train_data['realtitle'] = train_data['title']+train_data['tag']\n",
    "# test_data['realtitle'] = test_data['title']+test_data['tag']\n",
    "\n",
    "def getxx( x ):\n",
    "    if ctrmap.get(( x['prefix'],x['realtitle'] )) is  not None:\n",
    "        return ctrmap[( x['prefix'],x['realtitle'] )]\n",
    "    else:\n",
    "        return (alpha)/(alpha+beta) \n",
    "# train_data['ph_pred_ctr'] = train_data.apply( getxx , axis =1 )\n",
    "# test_data['ph_pred_ctr'] = test_data.apply( getxx , axis =1  )\n",
    "# train_data['ph_pred_ctr'].to_csv('d:/predctr2.csv',index=False)\n",
    "real_test_data['smooth_pre_title_tag_ctr'] = real_test_data.apply( getxx , axis =1  )\n",
    "real_test_data['smooth_pre_title_tag_ctr'].to_csv('E:/competionfile/oppo/data/singleFeature/real_test_smooth_pre_title_tag_ctr.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 高级ctr 对原始ctr进行了点击率平滑过后的点击率:\n",
    "#!/usr/bin/python\n",
    "# coding=utf-8\n",
    "##这里是用贝叶斯进行点击率平滑\n",
    "# import numpy\n",
    "# import random\n",
    "# import scipy.special as special\n",
    " \n",
    "# class BayesianSmoothing(object):\n",
    "#     def __init__(self, alpha, beta):\n",
    "#         self.alpha = alpha\n",
    "#         self.beta = beta\n",
    " \n",
    "#     def sample(self, alpha, beta, num, imp_upperbound):\n",
    "#         sample = numpy.random.beta(alpha, beta, num)\n",
    "# #         print(sample)\n",
    "#         I = []\n",
    "#         C = []\n",
    "#         for clk_rt in sample:\n",
    "#             imp = imp_upperbound\n",
    "#             clk = imp * clk_rt\n",
    "#             I.append(imp)\n",
    "#             C.append(clk)\n",
    "# #         print(C)\n",
    "#         return I, C\n",
    " \n",
    "#     def update(self, imps, clks, iter_num, epsilon):\n",
    "#         for i in range(iter_num):\n",
    "#             print('cur iter'+str(i))\n",
    "#             new_alpha, new_beta = self.__fixed_point_iteration(imps, clks, self.alpha, self.beta)\n",
    "#             print('alpha {} beta{}'.format( new_alpha , new_beta ))\n",
    "#             if abs(new_alpha-self.alpha)<epsilon and abs(new_beta-self.beta)<epsilon:\n",
    "#                 break\n",
    "#             self.alpha = new_alpha\n",
    "#             self.beta = new_beta\n",
    "#             print('alpha {} beta{}'.format( self.alpha , self.beta ))\n",
    " \n",
    "#     def __fixed_point_iteration(self, imps, clks, alpha, beta):\n",
    "#         numerator_alpha = 0.0\n",
    "#         numerator_beta = 0.0\n",
    "#         denominator = 0.0\n",
    "#         for i in range(len(imps)):\n",
    "#             numerator_alpha += (special.digamma(clks[i]+alpha) - special.digamma(alpha))\n",
    "#             numerator_beta += (special.digamma(imps[i]-clks[i]+beta) - special.digamma(beta))\n",
    "#             denominator += (special.digamma(imps[i]+alpha+beta) - special.digamma(alpha+beta))\n",
    "#         return alpha*(numerator_alpha/denominator), beta*(numerator_beta/denominator)\n",
    "# train_data['realtitle'] = train_data['title']+train_data['tag']\n",
    "# kkk = train_data.groupby(['prefix','title'])\n",
    "# namelist = []\n",
    "# clicklist = []\n",
    "# countlist = []\n",
    "# for name , group in tqdm( kkk ):\n",
    "#     namelist.append(name) \n",
    "#     clicklist.append( len( group['label'][group['label'].astype(int) == 1]  ) ) \n",
    "#     countlist.append(len( group ))\n",
    "\n",
    "# bs = BayesianSmoothing(1, 1)\n",
    "# bs.update(countlist, clicklist, 1000, 0.0000000001)\n",
    "\n",
    "# ctrmap = {}\n",
    "# alpha = 0.45415064089651735 \n",
    "# beta = 0.7502334280754297\n",
    "# for i in range(len(clicklist)):\n",
    "#         ctrmap[namelist[i]] =  (clicklist[i]+alpha)/(countlist[i]+alpha+beta) \n",
    "\n",
    "##将平滑后的概率重新注入进去\n",
    "# train_data['realtitle'] = train_data['title']+train_data['tag']\n",
    "# test_data['realtitle'] = test_data['title']+test_data['tag']\n",
    "\n",
    "def getxx( x ):\n",
    "    if ctrmap.get(( x['prefix'],x['title'] )) is  not None:\n",
    "        return ctrmap[( x['prefix'],x['title'] )]\n",
    "    else:\n",
    "        return (alpha)/(alpha+beta) \n",
    "train_data['smooth_pre_title_ctr'] = train_data.apply( getxx , axis =1 )\n",
    "# test_data['ph_pred_ctr'] = test_data.apply( getxx , axis =1  )\n",
    "# train_data['ph_pred_ctr'].to_csv('d:/predctr2.csv',index=False)\n",
    "# real_test_data['smooth_pre_title_ctr'] = real_test_data.apply( getxx , axis =1  )\n",
    "# real_test_data['smooth_pre_title_ctr'].to_csv('E:/competionfile/oppo/data/singleFeature/real_test_smooth_pre_title_ctr.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##判断是否含有数字\n",
    "# contain_num\n",
    "import re\n",
    "# train_data['contain_num'] = test_data['prefix'].map( lambda x : 0 if  re.search('.*\\\\d+.*',x) else 1  )\n",
    "# test_data['contain_num'] = train_data['prefix'].map( lambda x : 0 if  re.search('.*\\\\d+.*',x) else 1  )\n",
    "# train_data['contain_num'].to_csv('E:/competionfile/oppo/data/singleFeature/contain_num.csv',index=False)\n",
    "# test_data['contain_num'].to_csv('E:/competionfile/oppo/data/singleFeature/test_contain_num.csv',index=False)\n",
    "\n",
    "real_test_data['contain_num'] = real_test_data['prefix'].map( lambda x : 0 if  re.search('.*\\\\d+.*',x) else 1  )\n",
    "real_test_data['contain_num'].to_csv('E:/competionfile/oppo/data/singleFeature/real_test_contain_num.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##判断是否含有英文\n",
    "test_data_containeng =  test_data[test_data[0].str.contains('[a-zA-Z]+')]\n",
    "train_data_containeng = train_data[train_data[0].str.contains('[a-zA-Z]+')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##是否含有特殊字符\n",
    "import re\n",
    "##可以看到有一些搜索词里面有奇奇怪怪的东西，看看有哪些奇奇怪怪的东西，找出含有特殊标点符号的搜索（这里暂时没做出来）\n",
    "valueset = set()\n",
    "for idx in train_data[train_data[1].isnull()][0].value_counts().index:\n",
    "    strre = \"[`~!@#$^&*()=|{}':;',\\\\[\\\\].<>/?~！@#￥……&*（）——|{}【】‘；：”“'。，、？]\"\n",
    "    for i in strre:\n",
    "        if i in idx:\n",
    "            valueset.add(i)\n",
    "test_data_contain_symbol =  test_data[0].map( lambda x : 1 if x in valueset else 0 )\n",
    "train_data_contain_symbol =  train_data[0].map( lambda x : 1 if x in valueset else 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 匹配搜索词和 文章的相似度（ 通过搜索词前缀 去匹配  由所有点击过该文章的搜索词的预测词概率最大的前三位预测词组成的文章 的相似度 ）\n",
    "import json\n",
    "def generateWz( item ):\n",
    "    prediction = item[1]\n",
    "    query = item[0]\n",
    "    totalstr = query + ' '\n",
    "    obj = json.loads( prediction )  \n",
    "    for kk in obj:\n",
    "        totalstr = totalstr + kk +' '\n",
    "    return totalstr\n",
    "train_data['wzcontent'] = train_data.apply( generateWz , axis = 1 )\n",
    "test_data['wzcontent'] = test_data.apply( generateWz , axis = 1 )\n",
    "\n",
    "##构造libffm所需要的dataformat\n",
    "import pandas  as pd\n",
    "\n",
    "##怎么划分训练集和测试集\n",
    "##训练集的要求 1 尽量包含完所有搜索词  先随便来试一下哦，不要管那么多\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "data = pd.read_table('E:/competionfile/oppo/new/oppo_round1_train_20180929/oppo_round1_train_20180929.txt',names=[0,1,2,3,4],header=None,encoding='utf8').astype(str)\n",
    "data = data[ data[4] != '音乐' ]\n",
    "cdata = pd.read_table('E:/competionfile/oppo/new/oppo_round1_train_20180929/oppo_round1_vali_20180929.txt',names=[0,1,2,3,4],header=None,encoding='utf8').astype(str)\n",
    "data['realname'] = data[2]+data[3]\n",
    "datagroup = data.groupby('realname')\n",
    "realdict = {}\n",
    "for name ,group in datagroup:\n",
    "    strset = set( group['wzcontent'] )\n",
    "    totalstr = ''\n",
    "    for i in strset:\n",
    "        totalstr = totalstr+i+' '\n",
    "    realdict[name]=totalstr\n",
    "data['realname'] = data[2]+data[3]\n",
    "data['query'] = data[0]\n",
    "##计算每个搜索前缀和文章内容的相关性\n",
    "ggr = data.groupby( ['query','realname'] )\n",
    "ggrset = set()\n",
    "for i , group in ggr:\n",
    "    ggrset.add( (i[0],i[1],realdict[ i[1] ]) )\n",
    "f = open('e:/ggrdicttxt.txt','w' ,encoding='utf-8')\n",
    "for i in ggrset:\n",
    "#     print(i)\n",
    "    f.writelines(   (i[0]+ ':' +i[1]+':'+i[2]+'\\n') ) \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\tkhoon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "d:\\Users\\tkhoon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "## 计算 搜索预测词第一名的相似度和所有点击过该文章的搜索预测词前两名所拼接起来的文章的相似度\n",
    "## todo 这个暂时先放在这儿一下，有点问题还解决不了，比如只出现过一次的文章，\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "train_data_click = train_data[ train_data['label'] == 1  ]\n",
    "train_data_click['realtitle'] = train_data_click['title']+train_data_click['tag']\n",
    "def predictstr2list( item ):\n",
    "    predictStr = item['query_prediction']\n",
    "    totallist = []\n",
    "    totallist.append( item['prefix'] )\n",
    "    if type( predictStr ) != str:\n",
    "        return []\n",
    "    obj = json.loads(predictStr)\n",
    "    curlist = []\n",
    "    for  i in obj:\n",
    "        curlist.append( ( i , float(obj[i]) ) )\n",
    "    curlist.sort( key = lambda x : x[1] , reverse = True )\n",
    "    listlen = len( curlist )\n",
    "    if listlen > 2:\n",
    "        listlen = 2\n",
    "    \n",
    "    for i in range(0,listlen):\n",
    "        totallist.append( curlist[i][0] ) \n",
    "    return ','.join(totallist)      \n",
    "train_data_click['predict_list'] = train_data_click.apply( predictstr2list , axis = 1 )\n",
    "\n",
    "from tqdm import tqdm\n",
    "title_group  = train_data_click.groupby('realtitle')\n",
    "realtitle_dict = {}\n",
    "for name , group in tqdm( title_group ):\n",
    "    predict_set = set()\n",
    "    for subnamem , subgroup in group.groupby('prefix'):\n",
    "        predict_set.add( subgroup.iloc[0]['predict_list'])\n",
    "    realtitle_dict[name]= ','.join( predict_set )    \n",
    "##这个暂时先放在这儿以后来做 ，因为有些问题可能还没有考虑好，\n",
    "##生成待计算列表\n",
    "def get_predict_title_titlecontent_tupe( item ):\n",
    "    return \n",
    "wait_cac_sim_list = []\n",
    "for name ,group in tqdm( train_data_click.groupby(['prefix','realtitle']) ):\n",
    "    item = group.iloc[0]\n",
    "    prefix = item['prefix']\n",
    "    predict = item['predict_list'].split(',')\n",
    "    if len( predict ) == 1:\n",
    "        predict = predict[0]\n",
    "    else:\n",
    "        predict = predict[1]\n",
    "    title = item['realtitle']\n",
    "    title_content = realtitle_dict[title]\n",
    "    wait_cac_sim_list.append(  prefix + '|:+|'+ title +  '|:+|'+predict + '|:+|' + title_content +'\\n' )\n",
    "    \n",
    "f = open( 'e:/competionfile/oppo/tempfile/predict_title_content_sim_temp.txt' ,'w',encoding='utf-8')\n",
    "for s in wait_cac_sim_list:\n",
    "    f.writelines( s )\n",
    "f .close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Users\\tkhoon\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"d:\\Users\\tkhoon\\Anaconda3\\lib\\site-packages\\tqdm\\_tqdm.py\", line 106, in run\n",
      "    if instance.miniters > 1 and \\\n",
      "AttributeError: 'tqdm' object has no attribute 'miniters'\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████| 330422/330422 [01:27<00:00, 3797.54it/s]\n"
     ]
    }
   ],
   "source": [
    "##求 前缀 - 文章 - tag 的平均点击率\n",
    "avgctr_map = {}\n",
    "for name , group in tqdm( train_data.groupby( ['prefix','title','tag'] ) ):\n",
    "    avgctr_map[name] = group['label'].mean()\n",
    "\n",
    "def getAvgCtr( item ):\n",
    "    return avgctr_map.get( ( item['prefix'],item['title'],item['tag']) , 0.37)\n",
    "train_data['prefix_title_tag_ctr'] = train_data.apply( getAvgCtr , axis=1 )\n",
    "test_data['prefix_title_tag_ctr'] = test_data.apply( getAvgCtr , axis=1 ) \n",
    "# train_data['prefix_title_tag_ctr'].to_csv('E:/competionfile/oppo/data/singleFeature/prefix_title_tag_ctr.csv',index=False)\n",
    "# test_data['prefix_title_tag_ctr'].to_csv('E:/competionfile/oppo/data/singleFeature/test_prefix_title_tag_ctr.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Users\\tkhoon\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"d:\\Users\\tkhoon\\Anaconda3\\lib\\site-packages\\tqdm\\_tqdm.py\", line 106, in run\n",
      "    if instance.miniters > 1 and \\\n",
      "AttributeError: 'tqdm' object has no attribute 'miniters'\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████| 314678/314678 [02:54<00:00, 1808.02it/s]\n"
     ]
    }
   ],
   "source": [
    "##求 前缀 - 文章 的平均点击率\n",
    "avgctr_map = {}\n",
    "for name , group in tqdm( train_data.groupby( ['prefix','title'] ) ):\n",
    "    avgctr_map[name] = group['label'].mean()\n",
    "\n",
    "def getAvgCtr( item ):\n",
    "    return avgctr_map.get( ( item['prefix'],item['title']) , 0.37)\n",
    "train_data['prefix_title_ctr'] = train_data.apply( getAvgCtr , axis=1 )\n",
    "# test_data['prefix_title_ctr'] = test_data.apply( getAvgCtr , axis=1 ) \n",
    "# train_data['prefix_title_ctr'].to_csv('E:/competionfile/oppo/data/singleFeature/prefix_title_ctr.csv',index=False)\n",
    "# test_data['prefix_title_ctr'].to_csv('E:/competionfile/oppo/data/singleFeature/test_prefix_title_ctr.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['prefix_title_tag_ctr'].to_csv('E:/competionfile/oppo/data/singleFeature/prefix_title_tag_ctr.csv',index=False)\n",
    "test_data['prefix_title_tag_ctr'].to_csv('E:/competionfile/oppo/data/singleFeature/test_prefix_title_tag_ctr.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 计算smooth_pre_title_ctr_rank\n",
    "smooth_pre_title_rank_map = {}\n",
    "# for name , group in tqdm( train_data.groupby('prefix') ):\n",
    "#     pre_title_ctr_list = []\n",
    "#     sub_map = {}\n",
    "#     for subname , subgroup in group.groupby('title'):\n",
    "#         item = subgroup.iloc[0]\n",
    "#         ctr  = item['smooth_pre_title_ctr']\n",
    "#         pre_title_ctr_list.append( ( ( name , subname ) , ctr ) )\n",
    "#     ## 进行排序\n",
    "#     pre_title_ctr_list.sort( key = lambda x : x[1] , reverse = True )\n",
    "#     ## 塞到map里面去\n",
    "#     for i in range(0,len(pre_title_ctr_list)):\n",
    "#         sub_map[ pre_title_ctr_list[i][0] ] = i\n",
    "#     smooth_pre_title_rank_map[name] = sub_map\n",
    "\n",
    "##查找每条记录自己的那个pre_titletag_ctr_rank \n",
    "def find_smooth_pre_title_ctr_rank( item ):\n",
    "    prefix = item['prefix']\n",
    "    realtitle  = item['title']\n",
    "    rank_map = smooth_pre_title_rank_map.get( prefix )\n",
    "    if rank_map is None:\n",
    "        # 先试一下为None的效果吧\n",
    "        return None\n",
    "    self_rank = rank_map.get( ( prefix , realtitle  ) , None )\n",
    "#     if self_rank is None:\n",
    "#         print( (prefix , realtitle) ) \n",
    "    return self_rank\n",
    "\n",
    "\n",
    "# train_data['pre_title_ctr_rank'] = train_data.apply( find_pre_title_ctr_rank , axis = 1 )\n",
    "# test_data['pre_title_ctr_rank'] = test_data.apply( find_pre_title_ctr_rank , axis = 1 )\n",
    "\n",
    "real_test_data['smooth_pre_title_ctr_rank'] = real_test_data.apply( find_smooth_pre_title_ctr_rank , axis = 1 )\n",
    "real_test_data['smooth_pre_title_ctr_rank'] = real_test_data['smooth_pre_title_ctr_rank']+1\n",
    "real_test_data['smooth_pre_title_ctr_rank'].to_csv('E:/competionfile/oppo/data/singleFeature/real_test_smooth_pre_title_ctr_rank.csv',index=False)\n",
    "# train_data['pre_title_ctr_rank'] = train_data['pre_title_ctr_rank']+1\n",
    "# test_data['pre_title_ctr_rank'] = test_data['pre_title_ctr_rank']+1\n",
    "# train_data['pre_title_ctr_rank'].to_csv('E:/competionfile/oppo/data/singleFeature/pre_title_ctr_rank.csv',index=False)\n",
    "# test_data['pre_title_ctr_rank'].to_csv('E:/competionfile/oppo/data/singleFeature/test_pre_title_ctr_rank.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 157082/157082 [09:36<00:00, 272.25it/s]\n"
     ]
    }
   ],
   "source": [
    "## 当前数据所对应的文章 在这个词所有推荐过的文章中的点击率排名\n",
    "pre_title_rank_map = {}\n",
    "for name , group in tqdm( train_data.groupby('prefix') ):\n",
    "    pre_title_ctr_list = []\n",
    "    sub_map = {}\n",
    "    for subname , subgroup in group.groupby('title'):\n",
    "        item = subgroup.iloc[0]\n",
    "        ctr  = item['prefix_title_ctr']\n",
    "        pre_title_ctr_list.append( ( ( name , subname ) , ctr ) )\n",
    "    ## 进行排序\n",
    "    pre_title_ctr_list.sort( key = lambda x : x[1] , reverse = True )\n",
    "    ## 塞到map里面去\n",
    "    for i in range(0,len(pre_title_ctr_list)):\n",
    "        sub_map[ pre_title_ctr_list[i][0] ] = i\n",
    "    pre_title_rank_map[name] = sub_map\n",
    "\n",
    "##查找每条记录自己的那个pre_titletag_ctr_rank \n",
    "def find_pre_title_ctr_rank( item ):\n",
    "    prefix = item['prefix']\n",
    "    realtitle  = item['title']\n",
    "    rank_map = pre_title_rank_map.get( prefix )\n",
    "    if rank_map is None:\n",
    "        # 先试一下为None的效果吧\n",
    "        return None\n",
    "    self_rank = rank_map.get( ( prefix , realtitle  ) , None )\n",
    "#     if self_rank is None:\n",
    "#         print( (prefix , realtitle) ) \n",
    "    return self_rank\n",
    "\n",
    "\n",
    "# train_data['pre_title_ctr_rank'] = train_data.apply( find_pre_title_ctr_rank , axis = 1 )\n",
    "# test_data['pre_title_ctr_rank'] = test_data.apply( find_pre_title_ctr_rank , axis = 1 )\n",
    "\n",
    "real_test_data['pre_title_ctr_rank'] = real_test_data.apply( find_pre_title_ctr_rank , axis = 1 )\n",
    "real_test_data['pre_title_ctr_rank'] = real_test_data['pre_title_ctr_rank']+1\n",
    "real_test_data['pre_title_ctr_rank'].to_csv('E:/competionfile/oppo/data/singleFeature/real_test_pre_title_ctr_rank.csv',index=False)\n",
    "# train_data['pre_title_ctr_rank'] = train_data['pre_title_ctr_rank']+1\n",
    "# test_data['pre_title_ctr_rank'] = test_data['pre_title_ctr_rank']+1\n",
    "# train_data['pre_title_ctr_rank'].to_csv('E:/competionfile/oppo/data/singleFeature/pre_title_ctr_rank.csv',index=False)\n",
    "# test_data['pre_title_ctr_rank'].to_csv('E:/competionfile/oppo/data/singleFeature/test_pre_title_ctr_rank.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 157082/157082 [04:21<00:00, 599.62it/s]\n"
     ]
    }
   ],
   "source": [
    "## 当前数据所对应的文章 在这个词所有推荐过的文章中的点击率排名\n",
    "pre_title_tag_rank_map = {}\n",
    "for name , group in tqdm( train_data.groupby('prefix') ):\n",
    "    pre_title_ctr_list = []\n",
    "    sub_map = {}\n",
    "    for subname , subgroup in group.groupby('realtitle'):\n",
    "        item = subgroup.iloc[0]\n",
    "        ctr  = item['prefix_title_tag_ctr']\n",
    "        pre_title_ctr_list.append( ( ( name , subname ) , ctr ) )\n",
    "    ## 进行排序\n",
    "    pre_title_ctr_list.sort( key = lambda x : x[1] , reverse = True )\n",
    "    ## 塞到map里面去\n",
    "    for i in range(0,len(pre_title_ctr_list)):\n",
    "        sub_map[ pre_title_ctr_list[i][0] ] = i\n",
    "    pre_title_tag_rank_map[name] = sub_map\n",
    "\n",
    "##查找每条记录自己的那个pre_titletag_ctr_rank \n",
    "def find_pre_title_ctr_rank( item ):\n",
    "    prefix = item['prefix']\n",
    "    realtitle  = item['realtitle']\n",
    "    rank_map = pre_title_tag_rank_map.get( prefix )\n",
    "    if rank_map is None:\n",
    "        # 先试一下为None的效果吧\n",
    "        return None\n",
    "    self_rank = rank_map.get( ( prefix , realtitle  ) , None )\n",
    "#     if self_rank is None:\n",
    "#         print( (prefix , realtitle) ) \n",
    "    return self_rank\n",
    "\n",
    "\n",
    "# train_data['pre_title_ctr_rank'] = train_data.apply( find_pre_title_ctr_rank , axis = 1 )\n",
    "# test_data['pre_title_ctr_rank'] = test_data.apply( find_pre_title_ctr_rank , axis = 1 )\n",
    "\n",
    "real_test_data['pre_title_tag_ctr_rank'] = real_test_data.apply( find_pre_title_ctr_rank , axis = 1 )\n",
    "real_test_data['pre_title_tag_ctr_rank'] = real_test_data['pre_title_ctr_rank']+1\n",
    "real_test_data['pre_title_tag_ctr_rank'].to_csv('E:/competionfile/oppo/data/singleFeature/real_test_pre_title_tag_ctr_rank.csv',index=False)\n",
    "# train_data['pre_title_ctr_rank'] = train_data['pre_title_ctr_rank']+1\n",
    "# test_data['pre_title_ctr_rank'] = test_data['pre_title_ctr_rank']+1\n",
    "# train_data['pre_title_ctr_rank'].to_csv('E:/competionfile/oppo/data/singleFeature/pre_title_ctr_rank.csv',index=False)\n",
    "# test_data['pre_title_ctr_rank'].to_csv('E:/competionfile/oppo/data/singleFeature/test_pre_title_ctr_rank.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11668, 8)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[ test_data['pre_title_ctr_rank'].isnull() ].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['pre_titletag_ctr_rank'].to_csv('E:/competionfile/oppo/data/singleFeature/pre_titletag_ctr_rank.csv',index=False)\n",
    "test_data['pre_titletag_ctr_rank'].to_csv('E:/competionfile/oppo/data/singleFeature/test_pre_titletag_ctr_rank.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 157082/157082 [00:23<00:00, 6790.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(45175, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  train_data['realtitle'] = train_data['title']+train_data['tag']\n",
    "# 是否是新词的标记  看一下出现次数小于2词的数据有多少\n",
    "pre_count_map = {} \n",
    "gr = train_data.groupby('prefix')\n",
    "for name , group in tqdm(gr):\n",
    "    pre_count_map[name] = len(group)\n",
    "\n",
    "\n",
    "train_data['prefix_count']  =  train_data['prefix'].map(lambda x : pre_count_map.get( x , 0 ) )\n",
    "train_data[train_data['prefix_count'] <=1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title_len 文章名字长度\n",
    "train_data['title_len'] = train_data['title'].map( lambda x : len(x) )\n",
    "test_data['title_len'] = test_data['title'].map( lambda x : len(x) )\n",
    "train_data['title_len'].to_csv('E:/competionfile/oppo/data/singleFeature/title_len.csv')\n",
    "test_data['title_len'].to_csv('E:/competionfile/oppo/data/singleFeature/test_title_len.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##计算和它相似的推荐词的点击率\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 其他基础特征\n",
    "import json\n",
    "# prediction_3_avg_point 预测推荐词前三个的准确度的平均值\n",
    "def predictstr2list( predictStr ):\n",
    "    if type( predictStr ) != str or predictStr == '{}':\n",
    "        return 0\n",
    "    obj = json.loads(predictStr)\n",
    "    curlist = []\n",
    "    for  i in obj:\n",
    "        curlist.append( ( i , float(obj[i]) ) )\n",
    "    curlist.sort( key = lambda x : x[1] , reverse = True )\n",
    "    listlen = len( curlist )\n",
    "    if listlen > 3:\n",
    "        listlen = 3\n",
    "    total = 0 \n",
    "    totallen = 0\n",
    "    for i in range(0,listlen):\n",
    "        total+=curlist[i][1]\n",
    "        totallen+=1\n",
    "    return total/totallen     \n",
    "\n",
    "# train_data['prediction_3_avg_point'] = train_data['query_prediction'].map( predictstr2list  )\n",
    "# test_data['prediction_3_avg_point'] = test_data['query_prediction'].map( predictstr2list  )\n",
    "# train_data['prediction_3_avg_point'].to_csv('E:/competionfile/oppo/data/singleFeature/prediction_3_avg_point.csv',index=False)\n",
    "# test_data['prediction_3_avg_point'].to_csv('E:/competionfile/oppo/data/singleFeature/test_prediction_3_avg_point.csv',index=False)\n",
    "\n",
    "real_test_data['prediction_3_avg_point'] = real_test_data['query_prediction'].map( predictstr2list  )\n",
    "real_test_data['prediction_3_avg_point'].to_csv('E:/competionfile/oppo/data/singleFeature/real_testprediction_3_avg_point.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 其他基础特征\n",
    "import json\n",
    "# prediction_10_avg_point 预测推荐词前10个的准确度的平均值\n",
    "def predictstr2list( predictStr ):\n",
    "    if type( predictStr ) != str or predictStr == '{}':\n",
    "        return 0\n",
    "    obj = json.loads(predictStr)\n",
    "    curlist = []\n",
    "    for  i in obj:\n",
    "        curlist.append( ( i , float(obj[i]) ) )\n",
    "    curlist.sort( key = lambda x : x[1] , reverse = True )\n",
    "    listlen = len( curlist )\n",
    "    if listlen > 9:\n",
    "        listlen = 10\n",
    "    total = 0 \n",
    "    totallen = 0\n",
    "    for i in range(0,listlen):\n",
    "        total+=curlist[i][1]\n",
    "        totallen+=1\n",
    "    return total/totallen     \n",
    "\n",
    "# train_data['prediction_10_avg_point'] = train_data['query_prediction'].map( predictstr2list  )\n",
    "# test_data['prediction_10_avg_point'] = test_data['query_prediction'].map( predictstr2list  )\n",
    "# train_data['prediction_10_avg_point'].to_csv('E:/competionfile/oppo/data/singleFeature/prediction_10_avg_point.csv',index=False)\n",
    "# test_data['prediction_10_avg_point'].to_csv('E:/competionfile/oppo/data/singleFeature/test_prediction_10_avg_point.csv',index=False)\n",
    "\n",
    "real_test_data['prediction_10_avg_point'] = real_test_data['query_prediction'].map( predictstr2list  )\n",
    "real_test_data['prediction_10_avg_point'].to_csv('E:/competionfile/oppo/data/singleFeature/real_testprediction_10_avg_point.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 其他基础特征\n",
    "import json\n",
    "# prediction_1_avg_point 预测推荐词准确度最高的那个的值\n",
    "def predictstr2list( predictStr ):\n",
    "    if type( predictStr ) != str or predictStr == '{}':\n",
    "        return 0\n",
    "    obj = json.loads(predictStr)\n",
    "    curlist = []\n",
    "    for  i in obj:\n",
    "        curlist.append( ( i , float(obj[i]) ) )\n",
    "    curlist.sort( key = lambda x : x[1] , reverse = True )\n",
    "#     listlen = len( curlist )\n",
    "#     if listlen > 9:\n",
    "#         listlen = 10\n",
    "#     total = 0 \n",
    "#     totallen = 0\n",
    "#     for i in range(0,listlen):\n",
    "#         total+=curlist[i][1]\n",
    "#         totallen+=1\n",
    "    return curlist[0][1]    \n",
    "\n",
    "# train_data['prediction_1_avg_point'] = train_data['query_prediction'].map( predictstr2list  )\n",
    "# test_data['prediction_1_avg_point'] = test_data['query_prediction'].map( predictstr2list  )\n",
    "# train_data['prediction_1_avg_point'].to_csv('E:/competionfile/oppo/data/singleFeature/prediction_1_avg_point.csv',index=False)\n",
    "# test_data['prediction_1_avg_point'].to_csv('E:/competionfile/oppo/data/singleFeature/test_prediction_1_avg_point.csv',index=False)\n",
    "\n",
    "real_test_data['prediction_1_avg_point'] = real_test_data['query_prediction'].map( predictstr2list  )\n",
    "real_test_data['prediction_1_avg_point'].to_csv('E:/competionfile/oppo/data/singleFeature/real_testprediction_1_avg_point.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 判断 prediction 列表是否为空\n",
    "train_data['prediction_isnull'] = train_data['query_prediction'].map(lambda x : x == '{}')\n",
    "test_data['prediction_isnull'] = test_data['query_prediction'].map(lambda x : x == '{}')\n",
    "train_data['prediction_isnull'].to_csv('E:/competionfile/oppo/data/singleFeature/prediction_isnull.csv',index=False)\n",
    "test_data['prediction_isnull'].to_csv('E:/competionfile/oppo/data/singleFeature/test_prediction_isnull.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# ##计算词性\n",
    "from pyhanlp import *\n",
    "ct =0 \n",
    "total = real_test_data.shape[0]\n",
    "curpercent=0\n",
    "prepercent=0\n",
    "def findDCnum( item ):\n",
    "    cclist = []\n",
    "    global ct\n",
    "    global total \n",
    "    global curpercent \n",
    "    global prepercent\n",
    "    for term in HanLP.segment(item):\n",
    "        cclist.append( str(term.nature) )\n",
    "    ct+=1\n",
    "    curpercent = int( ct/total*100 )\n",
    "    if curpercent != prepercent:\n",
    "        prepercent = curpercent\n",
    "        print(curpercent)\n",
    "    return str( cclist )\n",
    "# ##对于每一个搜索词，求出他所包含的动词的数量，\n",
    "# train_data['cixing'] = train_data['prefix'].map( findDCnum )\n",
    "# test_data['cixing'] = test_data['prefix'].map( findDCnum )\n",
    "real_test_data['cixing'] = real_test_data['prefix'].map( findDCnum )\n",
    "# ##统计共有多少种词性的词\n",
    "# # data['cx']=kkk\n",
    "for qq in range(97,123):\n",
    "    i = chr(qq)\n",
    "    def findcxcs( item ):\n",
    "        global i\n",
    "        cc=0\n",
    "        reali = item.replace(\"'\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\" \",\"\").split(',')\n",
    "        for tt in reali:\n",
    "            if tt.startswith(i):\n",
    "                cc+=1\n",
    "        return cc\n",
    "#     train_data[i]=train_data['cixing'].map( findcxcs  )\n",
    "#     test_data[i]=test_data['cixing'].map( findcxcs  )\n",
    "    real_test_data[i]=real_test_data['cixing'].map( findcxcs  )\n",
    "    \n",
    "for qq in range(97,123):\n",
    "    i = chr(qq)\n",
    "    if (  i in['a','m','n','v'] ):\n",
    "#         print( '词性 {} 为1 数量 {}'.format( i , real_test_data[ real_test_data[i] >= 1 ].shape ) )\n",
    "#         train_data[i].to_csv('E:/competionfile/oppo/data/singleFeature/cixing-'+i+'.csv',index=False)\n",
    "#         test_data[i].to_csv('E:/competionfile/oppo/data/singleFeature/test_cixing-'+i+'.csv',index=False)\n",
    "        real_test_data[i].to_csv('E:/competionfile/oppo/data/singleFeature/real_test_cixing-'+i+'.csv',index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 把数据都分成新数据和老数据部分，计算新数据中 预测词曾经在老数据中出现过的那些相关联的数据的平均点击率\n",
    "prediction_map = {}\n",
    "# ## 找一下这些生词的预测词里面的项目有多少是在训练集中出现过的\n",
    "for item in tqdm( train_data.itertuples() ):\n",
    "#     print( item )\n",
    "    prediction_str = item.query_prediction\n",
    "    obj = json.loads( prediction_str )\n",
    "    if len( obj ) == 0:\n",
    "        continue\n",
    "    for pre_str in obj:\n",
    "        pre_str_list = prediction_map.get( pre_str , [] )\n",
    "        pre_str_list.append(item.Index)\n",
    "        if len( pre_str_list ) == 1:\n",
    "            prediction_map[pre_str] = pre_str_list\n",
    "def find_ocr_status( predictions ):\n",
    "    obj = json.loads( predictions )\n",
    "    if len(obj) == 0:\n",
    "        return 0\n",
    "    for prediction in obj:\n",
    "        if prediction_map.get(prediction) is not None:\n",
    "            return 1\n",
    "    return 0\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
